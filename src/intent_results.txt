Running test for model: rnn
main.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(args.load_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_intent.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_fin2.xml', hidden_size=4, in_dist=False, infer=False, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/rnn_best_model.pth', min_delta=0.001, model_type='rnn', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=True, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=9663, val=2040, test=2050
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Running test and predict...
Overall Test Results:
mae: 5.8851
rmse: 13.3125
non_zero_mae: 19.6914
non_zero_rmse: 27.9452
intent_accuracy: 0.8249
avg_test_loss: 180.3949

Intent-specific metrics:
Intent: intent_0
  mae: 6.4860
  rmse: 14.1669
  non_zero_mae: 19.8007
  non_zero_rmse: 28.8054
  precision: 0.4238
  recall: 1.0000
  f1_score: 0.5953
  support: 264.0000
Intent: intent_1
  mae: 5.6170
  rmse: 13.1001
  non_zero_mae: 20.7121
  non_zero_rmse: 29.3753
  precision: 1.0000
  recall: 0.7073
  f1_score: 0.8286
  support: 328.0000
Intent: intent_2
  mae: 5.4202
  rmse: 13.2253
  non_zero_mae: 19.4545
  non_zero_rmse: 27.1371
  precision: 1.0000
  recall: 0.7896
  f1_score: 0.8825
  support: 309.0000
Intent: intent_3
  mae: 6.6224
  rmse: 13.2074
  non_zero_mae: 18.1818
  non_zero_rmse: 25.9044
  precision: 1.0000
  recall: 0.8864
  f1_score: 0.9398
  support: 308.0000
Intent: intent_4
  mae: 5.7338
  rmse: 13.6260
  non_zero_mae: 20.5547
  non_zero_rmse: 29.5595
  precision: 1.0000
  recall: 0.7809
  f1_score: 0.8770
  support: 283.0000
Intent: intent_5
  mae: 5.9386
  rmse: 12.7898
  non_zero_mae: 18.8302
  non_zero_rmse: 26.5283
  precision: 1.0000
  recall: 0.8347
  f1_score: 0.9099
  support: 248.0000
Intent: intent_6
  mae: 5.4834
  rmse: 13.0938
  non_zero_mae: 20.5396
  non_zero_rmse: 28.4533
  precision: 1.0000
  recall: 0.8065
  f1_score: 0.8929
  support: 310.0000



Running test for model: lstm
main.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(args.load_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_intent.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_fin2.xml', hidden_size=4, in_dist=False, infer=False, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/lstm_best_model.pth', min_delta=0.001, model_type='lstm', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=True, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=9663, val=2040, test=2050
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Running test and predict...
Overall Test Results:
mae: 5.9621
rmse: 13.6353
non_zero_mae: 20.1421
non_zero_rmse: 28.8676
intent_accuracy: 0.8263
avg_test_loss: 189.8260

Intent-specific metrics:
Intent: intent_0
  mae: 6.4866
  rmse: 14.3568
  non_zero_mae: 20.2917
  non_zero_rmse: 29.4488
  precision: 0.4258
  recall: 1.0000
  f1_score: 0.5973
  support: 264.0000
Intent: intent_1
  mae: 5.5732
  rmse: 12.9453
  non_zero_mae: 20.5112
  non_zero_rmse: 29.0760
  precision: 1.0000
  recall: 0.7073
  f1_score: 0.8286
  support: 328.0000
Intent: intent_2
  mae: 5.6138
  rmse: 13.4706
  non_zero_mae: 19.7961
  non_zero_rmse: 27.9829
  precision: 1.0000
  recall: 0.7896
  f1_score: 0.8825
  support: 309.0000
Intent: intent_3
  mae: 6.4817
  rmse: 13.1518
  non_zero_mae: 18.6141
  non_zero_rmse: 26.0212
  precision: 1.0000
  recall: 0.8864
  f1_score: 0.9398
  support: 308.0000
Intent: intent_4
  mae: 5.9185
  rmse: 14.6719
  non_zero_mae: 21.8421
  non_zero_rmse: 32.2361
  precision: 1.0000
  recall: 0.7809
  f1_score: 0.8770
  support: 283.0000
Intent: intent_5
  mae: 6.1166
  rmse: 13.9125
  non_zero_mae: 19.9999
  non_zero_rmse: 29.3360
  precision: 1.0000
  recall: 0.8347
  f1_score: 0.9099
  support: 248.0000
Intent: intent_6
  mae: 5.6742
  rmse: 13.1360
  non_zero_mae: 20.3425
  non_zero_rmse: 28.4875
  precision: 1.0000
  recall: 0.8161
  f1_score: 0.8988
  support: 310.0000



Running test for model: vanillatf
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
main.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(args.load_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_intent.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_fin2.xml', hidden_size=4, in_dist=False, infer=False, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/vanillatf_best_model.pth', min_delta=0.001, model_type='vanillatf', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=True, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=9663, val=2040, test=2050
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Running test and predict...
Overall Test Results:
mae: 5.9446
rmse: 13.0165
non_zero_mae: 19.4724
non_zero_rmse: 27.0947
intent_accuracy: 0.8327
avg_test_loss: 171.9174

Intent-specific metrics:
Intent: intent_0
  mae: 6.4262
  rmse: 13.1015
  non_zero_mae: 18.9579
  non_zero_rmse: 26.1546
  precision: 1.0000
  recall: 0.8939
  f1_score: 0.9440
  support: 264.0000
Intent: intent_1
  mae: 5.7294
  rmse: 12.8718
  non_zero_mae: 19.9809
  non_zero_rmse: 28.4349
  precision: 1.0000
  recall: 0.7073
  f1_score: 0.8286
  support: 328.0000
Intent: intent_2
  mae: 5.6837
  rmse: 13.1204
  non_zero_mae: 19.3289
  non_zero_rmse: 26.7917
  precision: 1.0000
  recall: 0.7896
  f1_score: 0.8825
  support: 309.0000
Intent: intent_3
  mae: 6.5208
  rmse: 13.0663
  non_zero_mae: 18.6544
  non_zero_rmse: 25.7901
  precision: 1.0000
  recall: 0.8864
  f1_score: 0.9398
  support: 308.0000
Intent: intent_4
  mae: 5.8270
  rmse: 13.7473
  non_zero_mae: 21.0041
  non_zero_rmse: 29.6246
  precision: 1.0000
  recall: 0.7809
  f1_score: 0.8770
  support: 283.0000
Intent: intent_5
  mae: 5.9393
  rmse: 12.6364
  non_zero_mae: 18.7394
  non_zero_rmse: 26.1025
  precision: 0.4196
  recall: 1.0000
  f1_score: 0.5912
  support: 248.0000
Intent: intent_6
  mae: 5.5614
  rmse: 12.5478
  non_zero_mae: 19.8068
  non_zero_rmse: 26.8607
  precision: 1.0000
  recall: 0.8161
  f1_score: 0.8988
  support: 310.0000



Running test for model: tabbert
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
main.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(args.load_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_intent.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_fin2.xml', hidden_size=4, in_dist=False, infer=False, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabbert_best_model.pth', min_delta=0.001, model_type='tabbert', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=True, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=9663, val=2040, test=2050
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Running test and predict...
Overall Test Results:
mae: 6.1196
rmse: 13.7719
non_zero_mae: 19.8112
non_zero_rmse: 28.6590
intent_accuracy: 0.8327
avg_test_loss: 193.6881

Intent-specific metrics:
Intent: intent_0
  mae: 6.3875
  rmse: 12.9768
  non_zero_mae: 18.6085
  non_zero_rmse: 25.7317
  precision: 1.0000
  recall: 0.8939
  f1_score: 0.9440
  support: 264.0000
Intent: intent_1
  mae: 6.0935
  rmse: 15.2795
  non_zero_mae: 22.0480
  non_zero_rmse: 34.5728
  precision: 1.0000
  recall: 0.7073
  f1_score: 0.8286
  support: 328.0000
Intent: intent_2
  mae: 5.8513
  rmse: 13.6733
  non_zero_mae: 19.7957
  non_zero_rmse: 27.7623
  precision: 1.0000
  recall: 0.7896
  f1_score: 0.8825
  support: 309.0000
Intent: intent_3
  mae: 6.7796
  rmse: 13.3883
  non_zero_mae: 18.1989
  non_zero_rmse: 25.7934
  precision: 1.0000
  recall: 0.8864
  f1_score: 0.9398
  support: 308.0000
Intent: intent_4
  mae: 5.7391
  rmse: 13.2606
  non_zero_mae: 20.2469
  non_zero_rmse: 28.1981
  precision: 1.0000
  recall: 0.7809
  f1_score: 0.8770
  support: 283.0000
Intent: intent_5
  mae: 6.5106
  rmse: 15.1866
  non_zero_mae: 20.8899
  non_zero_rmse: 32.0456
  precision: 0.4196
  recall: 1.0000
  f1_score: 0.5912
  support: 248.0000
Intent: intent_6
  mae: 5.5654
  rmse: 12.4233
  non_zero_mae: 19.3400
  non_zero_rmse: 26.2697
  precision: 1.0000
  recall: 0.8161
  f1_score: 0.8988
  support: 310.0000



Running test for model: tabgpt2
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
main.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(args.load_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_intent.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_fin2.xml', hidden_size=4, in_dist=False, infer=False, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabgpt2_best_model.pth', min_delta=0.001, model_type='tabgpt2', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=True, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=9663, val=2040, test=2050
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Running test and predict...
Overall Test Results:
mae: 5.9090
rmse: 13.2318
non_zero_mae: 19.2435
non_zero_rmse: 27.2409
intent_accuracy: 0.8322
avg_test_loss: 178.3452

Intent-specific metrics:
Intent: intent_0
  mae: 6.4021
  rmse: 13.2589
  non_zero_mae: 18.8159
  non_zero_rmse: 26.1923
  precision: 1.0000
  recall: 0.8939
  f1_score: 0.9440
  support: 264.0000
Intent: intent_1
  mae: 5.6244
  rmse: 13.1613
  non_zero_mae: 19.8573
  non_zero_rmse: 28.8710
  precision: 1.0000
  recall: 0.7073
  f1_score: 0.8286
  support: 328.0000
Intent: intent_2
  mae: 5.6347
  rmse: 13.4625
  non_zero_mae: 19.4928
  non_zero_rmse: 27.1254
  precision: 1.0000
  recall: 0.7896
  f1_score: 0.8825
  support: 309.0000
Intent: intent_3
  mae: 6.6667
  rmse: 13.4617
  non_zero_mae: 18.1689
  non_zero_rmse: 26.0920
  precision: 0.9964
  recall: 0.8864
  f1_score: 0.9381
  support: 308.0000
Intent: intent_4
  mae: 5.5430
  rmse: 13.3773
  non_zero_mae: 20.3898
  non_zero_rmse: 28.7040
  precision: 1.0000
  recall: 0.7774
  f1_score: 0.8748
  support: 283.0000
Intent: intent_5
  mae: 6.0327
  rmse: 13.1337
  non_zero_mae: 18.6024
  non_zero_rmse: 26.7423
  precision: 0.4196
  recall: 1.0000
  f1_score: 0.5912
  support: 248.0000
Intent: intent_6
  mae: 5.5456
  rmse: 12.7567
  non_zero_mae: 19.5437
  non_zero_rmse: 27.0718
  precision: 1.0000
  recall: 0.8161
  f1_score: 0.8988
  support: 310.0000



Running test for model: tabllama
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
main.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(args.load_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/default_intent.yaml', debug=False, device='cuda:4', file_path='./multiple_scenario_data_fin2.xml', hidden_size=4, in_dist=False, infer=False, init_seq=1, intermediate_size=2400, learning_rate=2.491423889455553e-05, load_path='./training_results/tabllama_best_model.pth', min_delta=0.001, model_type='tabllama', noise_mean=0, noise_std=0.022400575875951165, num_epochs=300, num_heads=4, num_layers=2, patience=15, save_path='best_model.pth', seed=0, seq_num_heads=4, seq_num_layers=3, swap_prob=0.0083388429783301, test=True, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=9663, val=2040, test=2050
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Running test and predict...
Overall Test Results:
mae: 5.9685
rmse: 12.9236
non_zero_mae: 19.2162
non_zero_rmse: 26.8323
intent_accuracy: 0.8229
avg_test_loss: 169.4225

Intent-specific metrics:
Intent: intent_0
  mae: 6.4641
  rmse: 13.0786
  non_zero_mae: 18.6527
  non_zero_rmse: 25.8665
  precision: 0.4214
  recall: 0.9848
  f1_score: 0.5902
  support: 264.0000
Intent: intent_1
  mae: 5.6486
  rmse: 12.6915
  non_zero_mae: 20.0893
  non_zero_rmse: 28.3565
  precision: 0.9957
  recall: 0.7073
  f1_score: 0.8271
  support: 328.0000
Intent: intent_2
  mae: 5.7680
  rmse: 13.0372
  non_zero_mae: 18.9349
  non_zero_rmse: 26.4597
  precision: 1.0000
  recall: 0.7896
  f1_score: 0.8825
  support: 309.0000
Intent: intent_3
  mae: 6.7028
  rmse: 13.3318
  non_zero_mae: 18.6633
  non_zero_rmse: 26.2122
  precision: 0.9964
  recall: 0.8864
  f1_score: 0.9381
  support: 308.0000
Intent: intent_4
  mae: 5.7514
  rmse: 13.2238
  non_zero_mae: 20.1705
  non_zero_rmse: 28.2533
  precision: 0.9864
  recall: 0.7703
  f1_score: 0.8651
  support: 283.0000
Intent: intent_5
  mae: 5.9851
  rmse: 12.6546
  non_zero_mae: 18.7249
  non_zero_rmse: 26.1216
  precision: 1.0000
  recall: 0.8347
  f1_score: 0.9099
  support: 248.0000
Intent: intent_6
  mae: 5.5403
  rmse: 12.4346
  non_zero_mae: 19.3978
  non_zero_rmse: 26.5900
  precision: 0.9961
  recall: 0.8161
  f1_score: 0.8972
  support: 310.0000



