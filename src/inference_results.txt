Running test for model: rnn, init_seq: 1
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=1, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/rnn_best_model.pth', min_delta=0.001, model_type='rnn', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/rnn_best_model.pth
Overall Inference Results - MAE: 51.0367, RMSE: 102.5794, Non-zero MAE: 101.6358, Non-zero RMSE: 142.9680, Intent Accuracy: 0.2143
Position 1 - MAE: 11.3825, RMSE: 15.9352, Non-zero MAE: 38.2939, Non-zero RMSE: 46.1979, Intent Accuracy: 0.4286
Position 2 - MAE: 22.1476, RMSE: 35.9713, Non-zero MAE: 53.7719, Non-zero RMSE: 86.9941, Intent Accuracy: 0.6429
Position 3 - MAE: 14.7839, RMSE: 23.3645, Non-zero MAE: 45.4736, Non-zero RMSE: 63.3229, Intent Accuracy: 0.4286
Position 4 - MAE: 29.1283, RMSE: 45.3092, Non-zero MAE: 72.5488, Non-zero RMSE: 116.9771, Intent Accuracy: 0.5000
Position 5 - MAE: 27.9949, RMSE: 43.7900, Non-zero MAE: 71.6104, Non-zero RMSE: 114.1694, Intent Accuracy: 0.4286
Position 6 - MAE: 39.5950, RMSE: 59.3746, Non-zero MAE: 90.5836, Non-zero RMSE: 137.8876, Intent Accuracy: 0.5000
Position 7 - MAE: 39.2461, RMSE: 58.0009, Non-zero MAE: 88.2650, Non-zero RMSE: 134.0348, Intent Accuracy: 0.4524
Position 8 - MAE: 44.2723, RMSE: 65.6563, Non-zero MAE: 100.1370, Non-zero RMSE: 143.8501, Intent Accuracy: 0.4762
Position 9 - MAE: 44.7594, RMSE: 66.4168, Non-zero MAE: 100.1497, Non-zero RMSE: 142.3659, Intent Accuracy: 0.4286
Position 10 - MAE: 48.9374, RMSE: 72.2062, Non-zero MAE: 103.9642, Non-zero RMSE: 145.1403, Intent Accuracy: 0.3571
Position 11 - MAE: 50.7349, RMSE: 74.6500, Non-zero MAE: 103.2156, Non-zero RMSE: 145.3439, Intent Accuracy: 0.3333
Position 12 - MAE: 51.6954, RMSE: 75.9449, Non-zero MAE: 105.2182, Non-zero RMSE: 145.9203, Intent Accuracy: 0.3333
Position 13 - MAE: 53.7268, RMSE: 78.6890, Non-zero MAE: 109.5310, Non-zero RMSE: 149.5850, Intent Accuracy: 0.3095
Position 14 - MAE: 55.9693, RMSE: 81.8112, Non-zero MAE: 107.4925, Non-zero RMSE: 146.7243, Intent Accuracy: 0.2381
Position 15 - MAE: 57.1142, RMSE: 82.5035, Non-zero MAE: 108.9864, Non-zero RMSE: 146.5713, Intent Accuracy: 0.2143
Position 16 - MAE: 57.9413, RMSE: 82.8890, Non-zero MAE: 110.5366, Non-zero RMSE: 148.3139, Intent Accuracy: 0.2143
Position 17 - MAE: 60.1009, RMSE: 85.2308, Non-zero MAE: 111.7810, Non-zero RMSE: 149.8442, Intent Accuracy: 0.2143
Position 18 - MAE: 60.8961, RMSE: 86.5061, Non-zero MAE: 111.6151, Non-zero RMSE: 149.7525, Intent Accuracy: 0.1905
Position 19 - MAE: 61.9675, RMSE: 88.5165, Non-zero MAE: 115.3641, Non-zero RMSE: 154.0844, Intent Accuracy: 0.1667
Position 20 - MAE: 62.2020, RMSE: 88.9394, Non-zero MAE: 115.1932, Non-zero RMSE: 153.7845, Intent Accuracy: 0.2143
Position 21 - MAE: 62.9779, RMSE: 90.7543, Non-zero MAE: 118.3587, Non-zero RMSE: 156.9674, Intent Accuracy: 0.2143
Position 22 - MAE: 63.5122, RMSE: 91.9630, Non-zero MAE: 119.0978, Non-zero RMSE: 158.4180, Intent Accuracy: 0.1905
Position 23 - MAE: 63.6033, RMSE: 92.2786, Non-zero MAE: 117.1914, Non-zero RMSE: 156.4686, Intent Accuracy: 0.2857
Position 24 - MAE: 64.8662, RMSE: 93.9712, Non-zero MAE: 121.3113, Non-zero RMSE: 160.7631, Intent Accuracy: 0.2381
Position 25 - MAE: 65.3575, RMSE: 94.5773, Non-zero MAE: 119.3699, Non-zero RMSE: 159.8845, Intent Accuracy: 0.2381
Position 26 - MAE: 65.9464, RMSE: 95.6110, Non-zero MAE: 122.6660, Non-zero RMSE: 162.8949, Intent Accuracy: 0.2143
Position 27 - MAE: 66.3089, RMSE: 96.2139, Non-zero MAE: 123.0758, Non-zero RMSE: 163.8774, Intent Accuracy: 0.2143
Position 28 - MAE: 65.9741, RMSE: 95.7916, Non-zero MAE: 123.5472, Non-zero RMSE: 163.8388, Intent Accuracy: 0.2381
Position 29 - MAE: 65.3521, RMSE: 94.5803, Non-zero MAE: 122.4585, Non-zero RMSE: 162.5021, Intent Accuracy: 0.2143

Execution time: 2.23 seconds
Max GPU memory usage: 20.29 MB

----------------------------------------

Running test for model: lstm, init_seq: 1
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=1, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/lstm_best_model.pth', min_delta=0.001, model_type='lstm', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/lstm_best_model.pth
Overall Inference Results - MAE: 8.8265, RMSE: 24.9062, Non-zero MAE: 38.1612, Non-zero RMSE: 51.9493, Intent Accuracy: 0.1429
Position 1 - MAE: 11.7014, RMSE: 16.7994, Non-zero MAE: 47.0516, Non-zero RMSE: 55.2470, Intent Accuracy: 0.2857
Position 2 - MAE: 11.9345, RMSE: 18.6093, Non-zero MAE: 52.1980, Non-zero RMSE: 67.8732, Intent Accuracy: 0.7381
Position 3 - MAE: 12.4574, RMSE: 19.5292, Non-zero MAE: 55.0484, Non-zero RMSE: 72.1165, Intent Accuracy: 0.0238
Position 4 - MAE: 12.1819, RMSE: 18.8748, Non-zero MAE: 54.5145, Non-zero RMSE: 71.0426, Intent Accuracy: 0.0238
Position 5 - MAE: 11.8437, RMSE: 18.6814, Non-zero MAE: 52.0553, Non-zero RMSE: 68.5876, Intent Accuracy: 0.1429
Position 6 - MAE: 11.6635, RMSE: 18.9004, Non-zero MAE: 51.3106, Non-zero RMSE: 68.3858, Intent Accuracy: 0.1429
Position 7 - MAE: 11.4238, RMSE: 17.9634, Non-zero MAE: 48.5014, Non-zero RMSE: 64.4579, Intent Accuracy: 0.1429
Position 8 - MAE: 10.8498, RMSE: 16.9610, Non-zero MAE: 47.5149, Non-zero RMSE: 62.3248, Intent Accuracy: 0.1429
Position 9 - MAE: 10.5633, RMSE: 16.6988, Non-zero MAE: 46.5036, Non-zero RMSE: 61.7557, Intent Accuracy: 0.1429
Position 10 - MAE: 10.2327, RMSE: 16.0005, Non-zero MAE: 44.9525, Non-zero RMSE: 59.5292, Intent Accuracy: 0.1429
Position 11 - MAE: 10.0911, RMSE: 15.7940, Non-zero MAE: 43.3214, Non-zero RMSE: 56.5351, Intent Accuracy: 0.1429
Position 12 - MAE: 9.8476, RMSE: 15.5274, Non-zero MAE: 42.7354, Non-zero RMSE: 57.2527, Intent Accuracy: 0.1429
Position 13 - MAE: 9.4152, RMSE: 14.8919, Non-zero MAE: 41.2248, Non-zero RMSE: 54.4168, Intent Accuracy: 0.1429
Position 14 - MAE: 9.2023, RMSE: 14.6568, Non-zero MAE: 39.9927, Non-zero RMSE: 53.5239, Intent Accuracy: 0.1429
Position 15 - MAE: 8.6924, RMSE: 13.4506, Non-zero MAE: 38.0953, Non-zero RMSE: 49.5049, Intent Accuracy: 0.1429
Position 16 - MAE: 8.8383, RMSE: 13.7460, Non-zero MAE: 37.4685, Non-zero RMSE: 49.3937, Intent Accuracy: 0.1429
Position 17 - MAE: 8.3095, RMSE: 13.1185, Non-zero MAE: 36.2225, Non-zero RMSE: 47.8861, Intent Accuracy: 0.1429
Position 18 - MAE: 8.1200, RMSE: 12.8894, Non-zero MAE: 34.8668, Non-zero RMSE: 46.4316, Intent Accuracy: 0.1429
Position 19 - MAE: 7.5909, RMSE: 12.1596, Non-zero MAE: 32.9338, Non-zero RMSE: 43.3978, Intent Accuracy: 0.1429
Position 20 - MAE: 7.3534, RMSE: 11.6033, Non-zero MAE: 31.9862, Non-zero RMSE: 42.1610, Intent Accuracy: 0.1429
Position 21 - MAE: 7.0915, RMSE: 11.0555, Non-zero MAE: 30.8463, Non-zero RMSE: 40.3070, Intent Accuracy: 0.1429
Position 22 - MAE: 6.7423, RMSE: 10.6849, Non-zero MAE: 29.6868, Non-zero RMSE: 39.6531, Intent Accuracy: 0.1429
Position 23 - MAE: 6.4270, RMSE: 10.0248, Non-zero MAE: 27.3523, Non-zero RMSE: 35.9469, Intent Accuracy: 0.1429
Position 24 - MAE: 6.2230, RMSE: 9.8217, Non-zero MAE: 26.8090, Non-zero RMSE: 35.4097, Intent Accuracy: 0.1429
Position 25 - MAE: 5.8568, RMSE: 9.1840, Non-zero MAE: 25.4773, Non-zero RMSE: 34.0035, Intent Accuracy: 0.1429
Position 26 - MAE: 5.6355, RMSE: 8.9303, Non-zero MAE: 24.1572, Non-zero RMSE: 32.1553, Intent Accuracy: 0.1429
Position 27 - MAE: 5.2740, RMSE: 8.3901, Non-zero MAE: 22.7008, Non-zero RMSE: 30.4182, Intent Accuracy: 0.1429
Position 28 - MAE: 4.9423, RMSE: 7.8037, Non-zero MAE: 21.3862, Non-zero RMSE: 28.6987, Intent Accuracy: 0.1429
Position 29 - MAE: 4.6687, RMSE: 7.4322, Non-zero MAE: 20.1722, Non-zero RMSE: 26.8739, Intent Accuracy: 0.1429

Execution time: 2.54 seconds
Max GPU memory usage: 37.10 MB

----------------------------------------

Running test for model: vanillatf, init_seq: 1
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=1, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/vanillatf_best_model.pth', min_delta=0.001, model_type='vanillatf', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/vanillatf_best_model.pth
Overall Inference Results - MAE: 9.1568, RMSE: 22.5916, Non-zero MAE: 32.4084, Non-zero RMSE: 46.1911, Intent Accuracy: 0.1905
Position 1 - MAE: 11.7873, RMSE: 15.8399, Non-zero MAE: 26.7587, Non-zero RMSE: 35.5735, Intent Accuracy: 0.8333
Position 2 - MAE: 12.0827, RMSE: 17.8311, Non-zero MAE: 40.7884, Non-zero RMSE: 56.2347, Intent Accuracy: 0.7857
Position 3 - MAE: 11.7846, RMSE: 17.3877, Non-zero MAE: 44.1618, Non-zero RMSE: 59.0886, Intent Accuracy: 0.8333
Position 4 - MAE: 11.7608, RMSE: 17.4152, Non-zero MAE: 47.0099, Non-zero RMSE: 61.9295, Intent Accuracy: 0.6429
Position 5 - MAE: 11.3739, RMSE: 17.0726, Non-zero MAE: 45.0392, Non-zero RMSE: 60.1065, Intent Accuracy: 0.5714
Position 6 - MAE: 11.4386, RMSE: 17.9114, Non-zero MAE: 46.9199, Non-zero RMSE: 63.3459, Intent Accuracy: 0.4286
Position 7 - MAE: 11.2054, RMSE: 17.0640, Non-zero MAE: 44.1999, Non-zero RMSE: 59.5568, Intent Accuracy: 0.3810
Position 8 - MAE: 11.0826, RMSE: 16.7477, Non-zero MAE: 42.3667, Non-zero RMSE: 57.6118, Intent Accuracy: 0.2381
Position 9 - MAE: 10.7959, RMSE: 16.7051, Non-zero MAE: 41.5287, Non-zero RMSE: 57.3581, Intent Accuracy: 0.1667
Position 10 - MAE: 10.5834, RMSE: 16.0392, Non-zero MAE: 40.0014, Non-zero RMSE: 55.1401, Intent Accuracy: 0.2143
Position 11 - MAE: 10.5034, RMSE: 15.7539, Non-zero MAE: 38.2669, Non-zero RMSE: 52.1070, Intent Accuracy: 0.1429
Position 12 - MAE: 10.2110, RMSE: 15.6288, Non-zero MAE: 37.5199, Non-zero RMSE: 52.5951, Intent Accuracy: 0.2381
Position 13 - MAE: 9.7234, RMSE: 14.8931, Non-zero MAE: 36.5595, Non-zero RMSE: 49.8493, Intent Accuracy: 0.2619
Position 14 - MAE: 9.6661, RMSE: 15.0083, Non-zero MAE: 35.2766, Non-zero RMSE: 49.2899, Intent Accuracy: 0.2381
Position 15 - MAE: 9.1324, RMSE: 13.6765, Non-zero MAE: 32.9058, Non-zero RMSE: 44.7673, Intent Accuracy: 0.1905
Position 16 - MAE: 9.4126, RMSE: 14.2593, Non-zero MAE: 32.2935, Non-zero RMSE: 44.9284, Intent Accuracy: 0.1905
Position 17 - MAE: 8.9222, RMSE: 13.8311, Non-zero MAE: 31.9561, Non-zero RMSE: 44.1026, Intent Accuracy: 0.2143
Position 18 - MAE: 8.5734, RMSE: 13.2117, Non-zero MAE: 30.7501, Non-zero RMSE: 42.7146, Intent Accuracy: 0.2143
Position 19 - MAE: 8.2409, RMSE: 12.9120, Non-zero MAE: 28.6467, Non-zero RMSE: 39.6016, Intent Accuracy: 0.1667
Position 20 - MAE: 7.9921, RMSE: 12.2079, Non-zero MAE: 27.3639, Non-zero RMSE: 38.0497, Intent Accuracy: 0.2143
Position 21 - MAE: 7.7088, RMSE: 11.6504, Non-zero MAE: 26.2938, Non-zero RMSE: 36.0117, Intent Accuracy: 0.1905
Position 22 - MAE: 7.3330, RMSE: 11.2420, Non-zero MAE: 25.7274, Non-zero RMSE: 36.1710, Intent Accuracy: 0.1429
Position 23 - MAE: 6.9811, RMSE: 10.6560, Non-zero MAE: 23.2196, Non-zero RMSE: 32.2973, Intent Accuracy: 0.1667
Position 24 - MAE: 6.8409, RMSE: 10.3569, Non-zero MAE: 22.7901, Non-zero RMSE: 31.9371, Intent Accuracy: 0.1429
Position 25 - MAE: 6.5992, RMSE: 9.9923, Non-zero MAE: 20.9228, Non-zero RMSE: 30.1874, Intent Accuracy: 0.1667
Position 26 - MAE: 6.2136, RMSE: 9.4706, Non-zero MAE: 19.8126, Non-zero RMSE: 28.3008, Intent Accuracy: 0.1667
Position 27 - MAE: 5.9425, RMSE: 8.9978, Non-zero MAE: 18.7128, Non-zero RMSE: 26.9224, Intent Accuracy: 0.1429
Position 28 - MAE: 5.7437, RMSE: 8.5894, Non-zero MAE: 17.1807, Non-zero RMSE: 24.9539, Intent Accuracy: 0.1905
Position 29 - MAE: 5.3117, RMSE: 8.0093, Non-zero MAE: 16.3558, Non-zero RMSE: 23.5163, Intent Accuracy: 0.1905

Execution time: 2.96 seconds
Max GPU memory usage: 120.70 MB

----------------------------------------

Running test for model: tabbert, init_seq: 1
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=1, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabbert_best_model.pth', min_delta=0.001, model_type='tabbert', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabbert_best_model.pth
Overall Inference Results - MAE: 9.5280, RMSE: 22.8958, Non-zero MAE: 32.0300, Non-zero RMSE: 46.7212, Intent Accuracy: 0.1667
Position 1 - MAE: 11.6220, RMSE: 15.7974, Non-zero MAE: 29.5788, Non-zero RMSE: 38.2658, Intent Accuracy: 0.9762
Position 2 - MAE: 12.1036, RMSE: 18.4509, Non-zero MAE: 44.9933, Non-zero RMSE: 61.4678, Intent Accuracy: 0.7381
Position 3 - MAE: 12.7721, RMSE: 19.3667, Non-zero MAE: 47.8379, Non-zero RMSE: 65.2825, Intent Accuracy: 0.4762
Position 4 - MAE: 12.8554, RMSE: 19.3125, Non-zero MAE: 48.5124, Non-zero RMSE: 65.6501, Intent Accuracy: 0.3095
Position 5 - MAE: 12.5341, RMSE: 19.1510, Non-zero MAE: 46.3081, Non-zero RMSE: 63.5825, Intent Accuracy: 0.2619
Position 6 - MAE: 12.3428, RMSE: 19.2191, Non-zero MAE: 45.3409, Non-zero RMSE: 63.3457, Intent Accuracy: 0.1905
Position 7 - MAE: 12.0511, RMSE: 18.3677, Non-zero MAE: 42.5943, Non-zero RMSE: 59.4094, Intent Accuracy: 0.1905
Position 8 - MAE: 11.5412, RMSE: 17.4243, Non-zero MAE: 41.6281, Non-zero RMSE: 57.2248, Intent Accuracy: 0.1905
Position 9 - MAE: 11.1515, RMSE: 17.0307, Non-zero MAE: 40.5158, Non-zero RMSE: 56.7302, Intent Accuracy: 0.1905
Position 10 - MAE: 10.9261, RMSE: 16.5734, Non-zero MAE: 39.0767, Non-zero RMSE: 54.6255, Intent Accuracy: 0.1667
Position 11 - MAE: 10.7641, RMSE: 16.2257, Non-zero MAE: 37.6259, Non-zero RMSE: 51.5995, Intent Accuracy: 0.1667
Position 12 - MAE: 10.5369, RMSE: 16.0586, Non-zero MAE: 37.0042, Non-zero RMSE: 52.4563, Intent Accuracy: 0.1667
Position 13 - MAE: 10.1536, RMSE: 15.4463, Non-zero MAE: 35.5407, Non-zero RMSE: 49.5837, Intent Accuracy: 0.1667
Position 14 - MAE: 9.9884, RMSE: 15.3100, Non-zero MAE: 34.4035, Non-zero RMSE: 48.8917, Intent Accuracy: 0.1667
Position 15 - MAE: 9.4118, RMSE: 14.0455, Non-zero MAE: 32.4278, Non-zero RMSE: 44.7246, Intent Accuracy: 0.1667
Position 16 - MAE: 9.4801, RMSE: 14.1996, Non-zero MAE: 31.7928, Non-zero RMSE: 44.6071, Intent Accuracy: 0.1667
Position 17 - MAE: 9.1339, RMSE: 13.8469, Non-zero MAE: 30.6914, Non-zero RMSE: 43.1819, Intent Accuracy: 0.1667
Position 18 - MAE: 8.8907, RMSE: 13.5258, Non-zero MAE: 29.3669, Non-zero RMSE: 41.7950, Intent Accuracy: 0.1667
Position 19 - MAE: 8.3601, RMSE: 12.8079, Non-zero MAE: 27.3477, Non-zero RMSE: 38.7169, Intent Accuracy: 0.1667
Position 20 - MAE: 8.1092, RMSE: 12.2580, Non-zero MAE: 26.4786, Non-zero RMSE: 37.4497, Intent Accuracy: 0.1667
Position 21 - MAE: 7.8661, RMSE: 11.7451, Non-zero MAE: 25.3639, Non-zero RMSE: 35.6994, Intent Accuracy: 0.1667
Position 22 - MAE: 7.5818, RMSE: 11.5205, Non-zero MAE: 24.2095, Non-zero RMSE: 35.0167, Intent Accuracy: 0.1667
Position 23 - MAE: 7.1704, RMSE: 10.7407, Non-zero MAE: 22.0258, Non-zero RMSE: 31.4206, Intent Accuracy: 0.1667
Position 24 - MAE: 7.0467, RMSE: 10.6161, Non-zero MAE: 21.4726, Non-zero RMSE: 30.8705, Intent Accuracy: 0.1667
Position 25 - MAE: 6.7002, RMSE: 10.0885, Non-zero MAE: 20.2033, Non-zero RMSE: 29.5995, Intent Accuracy: 0.1667
Position 26 - MAE: 6.4895, RMSE: 9.8677, Non-zero MAE: 18.8917, Non-zero RMSE: 27.6633, Intent Accuracy: 0.1667
Position 27 - MAE: 6.1255, RMSE: 9.2479, Non-zero MAE: 17.4969, Non-zero RMSE: 26.1202, Intent Accuracy: 0.1667
Position 28 - MAE: 5.8465, RMSE: 8.8888, Non-zero MAE: 16.3081, Non-zero RMSE: 24.2689, Intent Accuracy: 0.1667
Position 29 - MAE: 5.6255, RMSE: 8.5705, Non-zero MAE: 15.1940, Non-zero RMSE: 22.5246, Intent Accuracy: 0.1667

Execution time: 2.76 seconds
Max GPU memory usage: 270.09 MB

----------------------------------------

Running test for model: tabgpt2, init_seq: 1
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=1, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabgpt2_best_model.pth', min_delta=0.001, model_type='tabgpt2', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabgpt2_best_model.pth
Overall Inference Results - MAE: 9.2258, RMSE: 23.8257, Non-zero MAE: 34.5312, Non-zero RMSE: 49.2347, Intent Accuracy: 0.1429
Position 1 - MAE: 11.6875, RMSE: 16.2048, Non-zero MAE: 36.4349, Non-zero RMSE: 46.0343, Intent Accuracy: 0.5476
Position 2 - MAE: 12.5442, RMSE: 19.2870, Non-zero MAE: 51.5953, Non-zero RMSE: 68.3704, Intent Accuracy: 0.5238
Position 3 - MAE: 12.8528, RMSE: 19.5650, Non-zero MAE: 50.6715, Non-zero RMSE: 68.3911, Intent Accuracy: 0.1905
Position 4 - MAE: 12.7086, RMSE: 19.2017, Non-zero MAE: 49.7930, Non-zero RMSE: 67.2494, Intent Accuracy: 0.1429
Position 5 - MAE: 12.2358, RMSE: 18.7864, Non-zero MAE: 48.1354, Non-zero RMSE: 65.4370, Intent Accuracy: 0.1190
Position 6 - MAE: 12.0235, RMSE: 19.0041, Non-zero MAE: 47.7730, Non-zero RMSE: 65.6913, Intent Accuracy: 0.1190
Position 7 - MAE: 11.8333, RMSE: 18.1899, Non-zero MAE: 44.1904, Non-zero RMSE: 61.2164, Intent Accuracy: 0.1190
Position 8 - MAE: 11.2207, RMSE: 17.1048, Non-zero MAE: 44.2814, Non-zero RMSE: 59.7718, Intent Accuracy: 0.1429
Position 9 - MAE: 11.0025, RMSE: 16.9350, Non-zero MAE: 43.0827, Non-zero RMSE: 59.0236, Intent Accuracy: 0.0714
Position 10 - MAE: 10.7841, RMSE: 16.4397, Non-zero MAE: 40.8641, Non-zero RMSE: 56.4716, Intent Accuracy: 0.1190
Position 11 - MAE: 10.3893, RMSE: 15.8416, Non-zero MAE: 40.0962, Non-zero RMSE: 54.0672, Intent Accuracy: 0.0714
Position 12 - MAE: 10.1901, RMSE: 15.7086, Non-zero MAE: 39.4752, Non-zero RMSE: 54.7891, Intent Accuracy: 0.0952
Position 13 - MAE: 9.9172, RMSE: 15.2801, Non-zero MAE: 37.4669, Non-zero RMSE: 51.6988, Intent Accuracy: 0.1667
Position 14 - MAE: 9.5963, RMSE: 14.7745, Non-zero MAE: 36.5778, Non-zero RMSE: 50.8917, Intent Accuracy: 0.1190
Position 15 - MAE: 9.0078, RMSE: 13.5825, Non-zero MAE: 35.0537, Non-zero RMSE: 47.1972, Intent Accuracy: 0.0714
Position 16 - MAE: 9.2461, RMSE: 13.9984, Non-zero MAE: 33.8966, Non-zero RMSE: 46.7741, Intent Accuracy: 0.2381
Position 17 - MAE: 8.6658, RMSE: 13.2513, Non-zero MAE: 32.7613, Non-zero RMSE: 45.2658, Intent Accuracy: 0.1905
Position 18 - MAE: 8.3968, RMSE: 12.9762, Non-zero MAE: 31.9475, Non-zero RMSE: 44.3011, Intent Accuracy: 0.0714
Position 19 - MAE: 8.0470, RMSE: 12.4583, Non-zero MAE: 29.0906, Non-zero RMSE: 40.5971, Intent Accuracy: 0.1667
Position 20 - MAE: 7.7444, RMSE: 11.7762, Non-zero MAE: 28.6082, Non-zero RMSE: 39.5544, Intent Accuracy: 0.1190
Position 21 - MAE: 7.4149, RMSE: 11.1829, Non-zero MAE: 27.9899, Non-zero RMSE: 38.0473, Intent Accuracy: 0.0952
Position 22 - MAE: 7.2336, RMSE: 11.1408, Non-zero MAE: 26.3010, Non-zero RMSE: 37.2444, Intent Accuracy: 0.1667
Position 23 - MAE: 6.7387, RMSE: 10.1287, Non-zero MAE: 23.9799, Non-zero RMSE: 33.4403, Intent Accuracy: 0.0714
Position 24 - MAE: 6.5109, RMSE: 9.9389, Non-zero MAE: 23.9669, Non-zero RMSE: 33.3578, Intent Accuracy: 0.0714
Position 25 - MAE: 6.3268, RMSE: 9.6089, Non-zero MAE: 22.1036, Non-zero RMSE: 31.7092, Intent Accuracy: 0.1905
Position 26 - MAE: 6.0176, RMSE: 9.1138, Non-zero MAE: 20.9185, Non-zero RMSE: 29.6887, Intent Accuracy: 0.0952
Position 27 - MAE: 5.5868, RMSE: 8.6247, Non-zero MAE: 20.1505, Non-zero RMSE: 28.5233, Intent Accuracy: 0.0952
Position 28 - MAE: 5.5045, RMSE: 8.4124, Non-zero MAE: 18.0881, Non-zero RMSE: 26.3530, Intent Accuracy: 0.1905
Position 29 - MAE: 5.0999, RMSE: 7.6626, Non-zero MAE: 17.1390, Non-zero RMSE: 24.5466, Intent Accuracy: 0.1429

Execution time: 4.33 seconds
Max GPU memory usage: 325.58 MB

----------------------------------------

Running test for model: tabllama, init_seq: 1
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/default_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=1, intermediate_size=2400, learning_rate=2.491423889455553e-05, load_path='./training_results/tabllama_best_model.pth', min_delta=0.001, model_type='tabllama', noise_mean=0, noise_std=0.022400575875951165, num_epochs=300, num_heads=4, num_layers=2, patience=15, save_path='best_model.pth', seed=0, seq_num_heads=4, seq_num_layers=3, swap_prob=0.0083388429783301, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabllama_best_model.pth
Overall Inference Results - MAE: 12.5560, RMSE: 24.7226, Non-zero MAE: 26.4176, Non-zero RMSE: 41.5328, Intent Accuracy: 0.1667
Position 1 - MAE: 11.5259, RMSE: 15.7211, Non-zero MAE: 30.2502, Non-zero RMSE: 38.6188, Intent Accuracy: 0.9286
Position 2 - MAE: 13.1566, RMSE: 19.0061, Non-zero MAE: 34.0361, Non-zero RMSE: 50.3545, Intent Accuracy: 0.5952
Position 3 - MAE: 14.8801, RMSE: 21.8877, Non-zero MAE: 38.3250, Non-zero RMSE: 56.8505, Intent Accuracy: 0.2857
Position 4 - MAE: 15.3506, RMSE: 22.7510, Non-zero MAE: 38.8202, Non-zero RMSE: 57.1532, Intent Accuracy: 0.2619
Position 5 - MAE: 15.0776, RMSE: 22.6535, Non-zero MAE: 37.5736, Non-zero RMSE: 56.2237, Intent Accuracy: 0.1905
Position 6 - MAE: 14.8723, RMSE: 22.6808, Non-zero MAE: 36.8068, Non-zero RMSE: 55.7783, Intent Accuracy: 0.1905
Position 7 - MAE: 14.6423, RMSE: 22.3638, Non-zero MAE: 34.2881, Non-zero RMSE: 52.2802, Intent Accuracy: 0.1190
Position 8 - MAE: 14.9009, RMSE: 22.7367, Non-zero MAE: 34.3964, Non-zero RMSE: 51.5806, Intent Accuracy: 0.1429
Position 9 - MAE: 14.4994, RMSE: 22.2098, Non-zero MAE: 33.9420, Non-zero RMSE: 51.6282, Intent Accuracy: 0.1667
Position 10 - MAE: 13.7916, RMSE: 20.7233, Non-zero MAE: 31.6818, Non-zero RMSE: 48.4358, Intent Accuracy: 0.1429
Position 11 - MAE: 13.8072, RMSE: 21.1510, Non-zero MAE: 30.4925, Non-zero RMSE: 45.5926, Intent Accuracy: 0.1190
Position 12 - MAE: 13.6675, RMSE: 21.1377, Non-zero MAE: 30.3055, Non-zero RMSE: 47.0891, Intent Accuracy: 0.1190
Position 13 - MAE: 13.1478, RMSE: 20.1306, Non-zero MAE: 28.6340, Non-zero RMSE: 43.5326, Intent Accuracy: 0.1905
Position 14 - MAE: 13.1705, RMSE: 20.3648, Non-zero MAE: 27.6205, Non-zero RMSE: 42.9152, Intent Accuracy: 0.1429
Position 15 - MAE: 12.7238, RMSE: 19.6436, Non-zero MAE: 26.5028, Non-zero RMSE: 39.9044, Intent Accuracy: 0.1190
Position 16 - MAE: 12.4015, RMSE: 19.0104, Non-zero MAE: 25.6253, Non-zero RMSE: 39.0306, Intent Accuracy: 0.1429
Position 17 - MAE: 12.2604, RMSE: 18.9899, Non-zero MAE: 24.5118, Non-zero RMSE: 37.4587, Intent Accuracy: 0.1190
Position 18 - MAE: 12.0703, RMSE: 18.4588, Non-zero MAE: 23.4239, Non-zero RMSE: 36.3814, Intent Accuracy: 0.1190
Position 19 - MAE: 11.7056, RMSE: 18.0563, Non-zero MAE: 22.0139, Non-zero RMSE: 34.0412, Intent Accuracy: 0.1429
Position 20 - MAE: 11.4986, RMSE: 17.8051, Non-zero MAE: 21.8784, Non-zero RMSE: 33.9911, Intent Accuracy: 0.0952
Position 21 - MAE: 11.1607, RMSE: 17.5274, Non-zero MAE: 20.0056, Non-zero RMSE: 31.0267, Intent Accuracy: 0.1905
Position 22 - MAE: 11.0867, RMSE: 17.5182, Non-zero MAE: 20.0343, Non-zero RMSE: 31.6167, Intent Accuracy: 0.1429
Position 23 - MAE: 10.6260, RMSE: 16.7905, Non-zero MAE: 18.0151, Non-zero RMSE: 28.0285, Intent Accuracy: 0.1429
Position 24 - MAE: 10.7701, RMSE: 16.9777, Non-zero MAE: 18.1555, Non-zero RMSE: 28.2939, Intent Accuracy: 0.1190
Position 25 - MAE: 10.6524, RMSE: 16.8474, Non-zero MAE: 17.3516, Non-zero RMSE: 27.6013, Intent Accuracy: 0.0952
Position 26 - MAE: 10.4622, RMSE: 16.9306, Non-zero MAE: 16.7098, Non-zero RMSE: 26.4920, Intent Accuracy: 0.1429
Position 27 - MAE: 10.3085, RMSE: 16.8951, Non-zero MAE: 15.8502, Non-zero RMSE: 25.4379, Intent Accuracy: 0.1667
Position 28 - MAE: 10.1564, RMSE: 16.7539, Non-zero MAE: 14.9751, Non-zero RMSE: 23.7370, Intent Accuracy: 0.1429
Position 29 - MAE: 10.2532, RMSE: 16.7159, Non-zero MAE: 14.4039, Non-zero RMSE: 22.9610, Intent Accuracy: 0.1667

Execution time: 3.64 seconds
Max GPU memory usage: 337.22 MB

----------------------------------------

Running test for model: rnn, init_seq: 2
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=2, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/rnn_best_model.pth', min_delta=0.001, model_type='rnn', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/rnn_best_model.pth
Overall Inference Results - MAE: 10.9528, RMSE: 31.2832, Non-zero MAE: 38.2911, Non-zero RMSE: 54.4347, Intent Accuracy: 0.0952
Position 2 - MAE: 12.1727, RMSE: 17.2931, Non-zero MAE: 42.2276, Non-zero RMSE: 50.9453, Intent Accuracy: 0.1429
Position 3 - MAE: 12.4824, RMSE: 19.2703, Non-zero MAE: 45.9622, Non-zero RMSE: 61.0751, Intent Accuracy: 0.1667
Position 4 - MAE: 13.5030, RMSE: 21.7519, Non-zero MAE: 42.7196, Non-zero RMSE: 58.8399, Intent Accuracy: 0.7143
Position 5 - MAE: 12.7448, RMSE: 19.7147, Non-zero MAE: 52.1858, Non-zero RMSE: 67.7163, Intent Accuracy: 0.2619
Position 6 - MAE: 13.4590, RMSE: 22.2566, Non-zero MAE: 51.3403, Non-zero RMSE: 69.5170, Intent Accuracy: 0.2381
Position 7 - MAE: 13.4322, RMSE: 23.3084, Non-zero MAE: 48.9471, Non-zero RMSE: 69.0803, Intent Accuracy: 0.2381
Position 8 - MAE: 13.1979, RMSE: 21.9463, Non-zero MAE: 48.8638, Non-zero RMSE: 67.4088, Intent Accuracy: 0.1190
Position 9 - MAE: 12.5268, RMSE: 20.3785, Non-zero MAE: 46.8438, Non-zero RMSE: 62.1301, Intent Accuracy: 0.0952
Position 10 - MAE: 12.1326, RMSE: 19.9767, Non-zero MAE: 45.8576, Non-zero RMSE: 62.3813, Intent Accuracy: 0.1667
Position 11 - MAE: 12.5084, RMSE: 21.5083, Non-zero MAE: 44.9474, Non-zero RMSE: 61.9803, Intent Accuracy: 0.1429
Position 12 - MAE: 13.1313, RMSE: 23.4132, Non-zero MAE: 44.1725, Non-zero RMSE: 60.7490, Intent Accuracy: 0.1429
Position 13 - MAE: 12.2933, RMSE: 21.6928, Non-zero MAE: 43.2501, Non-zero RMSE: 60.4823, Intent Accuracy: 0.1190
Position 14 - MAE: 11.5801, RMSE: 20.0730, Non-zero MAE: 41.3800, Non-zero RMSE: 56.8109, Intent Accuracy: 0.0714
Position 15 - MAE: 11.1997, RMSE: 19.6023, Non-zero MAE: 40.3054, Non-zero RMSE: 57.5296, Intent Accuracy: 0.0952
Position 16 - MAE: 10.5178, RMSE: 17.9474, Non-zero MAE: 38.7767, Non-zero RMSE: 53.2667, Intent Accuracy: 0.1190
Position 17 - MAE: 11.4208, RMSE: 20.4263, Non-zero MAE: 37.8353, Non-zero RMSE: 52.6361, Intent Accuracy: 0.1667
Position 18 - MAE: 10.1898, RMSE: 17.4680, Non-zero MAE: 35.4422, Non-zero RMSE: 49.2798, Intent Accuracy: 0.0952
Position 19 - MAE: 10.3920, RMSE: 18.6930, Non-zero MAE: 35.2879, Non-zero RMSE: 48.9025, Intent Accuracy: 0.0952
Position 20 - MAE: 9.5333, RMSE: 16.9039, Non-zero MAE: 34.0436, Non-zero RMSE: 48.3547, Intent Accuracy: 0.0714
Position 21 - MAE: 10.2246, RMSE: 19.2836, Non-zero MAE: 33.1355, Non-zero RMSE: 49.3147, Intent Accuracy: 0.1667
Position 22 - MAE: 10.1227, RMSE: 20.2377, Non-zero MAE: 32.7713, Non-zero RMSE: 49.4593, Intent Accuracy: 0.1190
Position 23 - MAE: 8.7686, RMSE: 15.8232, Non-zero MAE: 30.1205, Non-zero RMSE: 43.2144, Intent Accuracy: 0.0952
Position 24 - MAE: 7.8810, RMSE: 13.2623, Non-zero MAE: 28.3208, Non-zero RMSE: 40.7208, Intent Accuracy: 0.1190
Position 25 - MAE: 7.5526, RMSE: 12.7570, Non-zero MAE: 27.6343, Non-zero RMSE: 40.2060, Intent Accuracy: 0.1190
Position 26 - MAE: 8.2903, RMSE: 15.5204, Non-zero MAE: 26.9778, Non-zero RMSE: 41.1450, Intent Accuracy: 0.1429
Position 27 - MAE: 7.5191, RMSE: 13.7084, Non-zero MAE: 25.0079, Non-zero RMSE: 36.4740, Intent Accuracy: 0.1429
Position 28 - MAE: 7.6857, RMSE: 15.1662, Non-zero MAE: 24.4411, Non-zero RMSE: 39.1880, Intent Accuracy: 0.1190
Position 29 - MAE: 7.2314, RMSE: 14.4360, Non-zero MAE: 23.8964, Non-zero RMSE: 41.8854, Intent Accuracy: 0.0952

Execution time: 3.12 seconds
Max GPU memory usage: 21.02 MB

----------------------------------------

Running test for model: lstm, init_seq: 2
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=2, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/lstm_best_model.pth', min_delta=0.001, model_type='lstm', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/lstm_best_model.pth
Overall Inference Results - MAE: 8.9867, RMSE: 25.1566, Non-zero MAE: 38.6513, Non-zero RMSE: 52.4658, Intent Accuracy: 0.1429
Position 2 - MAE: 11.7821, RMSE: 16.9085, Non-zero MAE: 47.0567, Non-zero RMSE: 55.3649, Intent Accuracy: 0.2857
Position 3 - MAE: 12.3893, RMSE: 19.3871, Non-zero MAE: 53.6514, Non-zero RMSE: 70.2089, Intent Accuracy: 0.2857
Position 4 - MAE: 11.9131, RMSE: 18.4530, Non-zero MAE: 51.8664, Non-zero RMSE: 67.7722, Intent Accuracy: 0.6429
Position 5 - MAE: 12.1963, RMSE: 18.8884, Non-zero MAE: 54.4579, Non-zero RMSE: 71.0147, Intent Accuracy: 0.0000
Position 6 - MAE: 11.8802, RMSE: 18.7221, Non-zero MAE: 52.0203, Non-zero RMSE: 68.5934, Intent Accuracy: 0.0000
Position 7 - MAE: 11.6822, RMSE: 18.9167, Non-zero MAE: 51.2378, Non-zero RMSE: 68.3484, Intent Accuracy: 0.0238
Position 8 - MAE: 11.4369, RMSE: 17.9788, Non-zero MAE: 48.3975, Non-zero RMSE: 64.3754, Intent Accuracy: 0.0000
Position 9 - MAE: 10.8574, RMSE: 16.9714, Non-zero MAE: 47.4411, Non-zero RMSE: 62.2587, Intent Accuracy: 0.0952
Position 10 - MAE: 10.5657, RMSE: 16.7037, Non-zero MAE: 46.4008, Non-zero RMSE: 61.6689, Intent Accuracy: 0.0714
Position 11 - MAE: 10.2415, RMSE: 16.0077, Non-zero MAE: 44.8256, Non-zero RMSE: 59.4115, Intent Accuracy: 0.1429
Position 12 - MAE: 10.0977, RMSE: 15.8023, Non-zero MAE: 43.2300, Non-zero RMSE: 56.4546, Intent Accuracy: 0.1905
Position 13 - MAE: 9.8563, RMSE: 15.5356, Non-zero MAE: 42.6183, Non-zero RMSE: 57.1659, Intent Accuracy: 0.1429
Position 14 - MAE: 9.4364, RMSE: 14.9201, Non-zero MAE: 41.1124, Non-zero RMSE: 54.3199, Intent Accuracy: 0.1429
Position 15 - MAE: 9.2209, RMSE: 14.6768, Non-zero MAE: 39.8797, Non-zero RMSE: 53.4256, Intent Accuracy: 0.1429
Position 16 - MAE: 8.7085, RMSE: 13.4677, Non-zero MAE: 38.0049, Non-zero RMSE: 49.4161, Intent Accuracy: 0.1429
Position 17 - MAE: 8.8462, RMSE: 13.7524, Non-zero MAE: 37.3753, Non-zero RMSE: 49.3067, Intent Accuracy: 0.1429
Position 18 - MAE: 8.3270, RMSE: 13.1373, Non-zero MAE: 36.1445, Non-zero RMSE: 47.8224, Intent Accuracy: 0.1429
Position 19 - MAE: 8.1321, RMSE: 12.9024, Non-zero MAE: 34.7511, Non-zero RMSE: 46.3418, Intent Accuracy: 0.1429
Position 20 - MAE: 7.6018, RMSE: 12.1664, Non-zero MAE: 32.7951, Non-zero RMSE: 43.2875, Intent Accuracy: 0.1429
Position 21 - MAE: 7.3656, RMSE: 11.6136, Non-zero MAE: 31.8556, Non-zero RMSE: 42.0486, Intent Accuracy: 0.1429
Position 22 - MAE: 7.0998, RMSE: 11.0620, Non-zero MAE: 30.7442, Non-zero RMSE: 40.2197, Intent Accuracy: 0.1429
Position 23 - MAE: 6.7542, RMSE: 10.6989, Non-zero MAE: 29.5974, Non-zero RMSE: 39.5795, Intent Accuracy: 0.1190
Position 24 - MAE: 6.4437, RMSE: 10.0415, Non-zero MAE: 27.2481, Non-zero RMSE: 35.8677, Intent Accuracy: 0.1429
Position 25 - MAE: 6.2375, RMSE: 9.8357, Non-zero MAE: 26.6718, Non-zero RMSE: 35.2904, Intent Accuracy: 0.1429
Position 26 - MAE: 5.8736, RMSE: 9.2035, Non-zero MAE: 25.3564, Non-zero RMSE: 33.9005, Intent Accuracy: 0.1429
Position 27 - MAE: 5.6417, RMSE: 8.9341, Non-zero MAE: 24.0397, Non-zero RMSE: 32.0619, Intent Accuracy: 0.1429
Position 28 - MAE: 5.2851, RMSE: 8.4019, Non-zero MAE: 22.5978, Non-zero RMSE: 30.3324, Intent Accuracy: 0.1429
Position 29 - MAE: 4.9560, RMSE: 7.8182, Non-zero MAE: 21.2853, Non-zero RMSE: 28.6115, Intent Accuracy: 0.1429

Execution time: 3.07 seconds
Max GPU memory usage: 37.47 MB

----------------------------------------

Running test for model: vanillatf, init_seq: 2
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=2, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/vanillatf_best_model.pth', min_delta=0.001, model_type='vanillatf', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/vanillatf_best_model.pth
Overall Inference Results - MAE: 9.3279, RMSE: 22.8147, Non-zero MAE: 32.7125, Non-zero RMSE: 46.4217, Intent Accuracy: 0.1429
Position 2 - MAE: 11.6075, RMSE: 15.5808, Non-zero MAE: 26.8036, Non-zero RMSE: 35.4611, Intent Accuracy: 0.8810
Position 3 - MAE: 11.9800, RMSE: 17.7379, Non-zero MAE: 42.0382, Non-zero RMSE: 57.5997, Intent Accuracy: 0.4762
Position 4 - MAE: 11.8778, RMSE: 17.5485, Non-zero MAE: 44.6075, Non-zero RMSE: 60.3695, Intent Accuracy: 0.4286
Position 5 - MAE: 11.6041, RMSE: 16.9975, Non-zero MAE: 43.9034, Non-zero RMSE: 58.7170, Intent Accuracy: 0.5000
Position 6 - MAE: 11.5272, RMSE: 17.2532, Non-zero MAE: 44.4785, Non-zero RMSE: 59.9955, Intent Accuracy: 0.3571
Position 7 - MAE: 11.3079, RMSE: 17.3008, Non-zero MAE: 44.4551, Non-zero RMSE: 60.6544, Intent Accuracy: 0.3810
Position 8 - MAE: 11.1267, RMSE: 16.7002, Non-zero MAE: 42.5559, Non-zero RMSE: 57.8365, Intent Accuracy: 0.3571
Position 9 - MAE: 10.6565, RMSE: 16.0670, Non-zero MAE: 42.7609, Non-zero RMSE: 57.3376, Intent Accuracy: 0.3095
Position 10 - MAE: 10.5429, RMSE: 16.1076, Non-zero MAE: 41.9086, Non-zero RMSE: 56.9296, Intent Accuracy: 0.2381
Position 11 - MAE: 10.4271, RMSE: 15.7596, Non-zero MAE: 40.5009, Non-zero RMSE: 55.2831, Intent Accuracy: 0.3095
Position 12 - MAE: 10.5148, RMSE: 15.7426, Non-zero MAE: 37.7892, Non-zero RMSE: 51.6741, Intent Accuracy: 0.1905
Position 13 - MAE: 10.0066, RMSE: 15.1952, Non-zero MAE: 38.0720, Non-zero RMSE: 52.7252, Intent Accuracy: 0.1905
Position 14 - MAE: 9.7295, RMSE: 14.8879, Non-zero MAE: 36.3553, Non-zero RMSE: 49.9680, Intent Accuracy: 0.2143
Position 15 - MAE: 9.5137, RMSE: 14.4743, Non-zero MAE: 34.7151, Non-zero RMSE: 48.6582, Intent Accuracy: 0.2143
Position 16 - MAE: 8.8402, RMSE: 13.1746, Non-zero MAE: 33.4072, Non-zero RMSE: 44.9234, Intent Accuracy: 0.2143
Position 17 - MAE: 9.1484, RMSE: 13.6450, Non-zero MAE: 32.4144, Non-zero RMSE: 44.9701, Intent Accuracy: 0.1667
Position 18 - MAE: 8.7136, RMSE: 13.1865, Non-zero MAE: 31.3582, Non-zero RMSE: 43.4163, Intent Accuracy: 0.1905
Position 19 - MAE: 8.5687, RMSE: 13.0224, Non-zero MAE: 29.8174, Non-zero RMSE: 41.8386, Intent Accuracy: 0.1667
Position 20 - MAE: 8.0044, RMSE: 12.2492, Non-zero MAE: 28.5746, Non-zero RMSE: 39.2989, Intent Accuracy: 0.1905
Position 21 - MAE: 7.9135, RMSE: 12.0399, Non-zero MAE: 27.3089, Non-zero RMSE: 37.9891, Intent Accuracy: 0.2143
Position 22 - MAE: 7.6725, RMSE: 11.5642, Non-zero MAE: 26.4751, Non-zero RMSE: 36.2681, Intent Accuracy: 0.1667
Position 23 - MAE: 7.5352, RMSE: 11.6122, Non-zero MAE: 25.2204, Non-zero RMSE: 35.8106, Intent Accuracy: 0.1429
Position 24 - MAE: 7.0614, RMSE: 10.7646, Non-zero MAE: 23.0352, Non-zero RMSE: 32.2166, Intent Accuracy: 0.1667
Position 25 - MAE: 7.0381, RMSE: 10.7394, Non-zero MAE: 22.1431, Non-zero RMSE: 31.3850, Intent Accuracy: 0.1667
Position 26 - MAE: 6.5876, RMSE: 10.0711, Non-zero MAE: 21.1829, Non-zero RMSE: 29.9364, Intent Accuracy: 0.1667
Position 27 - MAE: 6.6523, RMSE: 10.1743, Non-zero MAE: 19.5982, Non-zero RMSE: 28.2461, Intent Accuracy: 0.1905
Position 28 - MAE: 6.1176, RMSE: 9.5141, Non-zero MAE: 18.7342, Non-zero RMSE: 26.9540, Intent Accuracy: 0.1667
Position 29 - MAE: 5.9730, RMSE: 9.1586, Non-zero MAE: 17.1869, Non-zero RMSE: 25.0255, Intent Accuracy: 0.1429

Execution time: 3.07 seconds
Max GPU memory usage: 122.11 MB

----------------------------------------

Running test for model: tabbert, init_seq: 2
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=2, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabbert_best_model.pth', min_delta=0.001, model_type='tabbert', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabbert_best_model.pth
Overall Inference Results - MAE: 9.6366, RMSE: 23.1166, Non-zero MAE: 32.5601, Non-zero RMSE: 47.2156, Intent Accuracy: 0.1667
Position 2 - MAE: 11.6186, RMSE: 15.7911, Non-zero MAE: 29.5673, Non-zero RMSE: 38.2549, Intent Accuracy: 0.9762
Position 3 - MAE: 12.1053, RMSE: 18.4754, Non-zero MAE: 45.2316, Non-zero RMSE: 61.7405, Intent Accuracy: 0.7381
Position 4 - MAE: 12.7831, RMSE: 19.3807, Non-zero MAE: 47.9708, Non-zero RMSE: 65.4401, Intent Accuracy: 0.5000
Position 5 - MAE: 12.7766, RMSE: 19.1827, Non-zero MAE: 48.3139, Non-zero RMSE: 65.2576, Intent Accuracy: 0.3810
Position 6 - MAE: 12.4865, RMSE: 19.0669, Non-zero MAE: 46.1536, Non-zero RMSE: 63.3365, Intent Accuracy: 0.3571
Position 7 - MAE: 12.2676, RMSE: 19.0793, Non-zero MAE: 45.3033, Non-zero RMSE: 63.1690, Intent Accuracy: 0.3333
Position 8 - MAE: 11.9544, RMSE: 18.1901, Non-zero MAE: 42.4792, Non-zero RMSE: 59.1360, Intent Accuracy: 0.3571
Position 9 - MAE: 11.4709, RMSE: 17.2859, Non-zero MAE: 41.4883, Non-zero RMSE: 56.9698, Intent Accuracy: 0.3095
Position 10 - MAE: 11.1148, RMSE: 16.9411, Non-zero MAE: 40.4133, Non-zero RMSE: 56.5327, Intent Accuracy: 0.3095
Position 11 - MAE: 10.8312, RMSE: 16.3637, Non-zero MAE: 38.9133, Non-zero RMSE: 54.3486, Intent Accuracy: 0.3333
Position 12 - MAE: 10.6796, RMSE: 16.0489, Non-zero MAE: 37.4524, Non-zero RMSE: 51.2556, Intent Accuracy: 0.2857
Position 13 - MAE: 10.4661, RMSE: 15.8966, Non-zero MAE: 36.8337, Non-zero RMSE: 52.1250, Intent Accuracy: 0.2619
Position 14 - MAE: 10.0911, RMSE: 15.3102, Non-zero MAE: 35.3674, Non-zero RMSE: 49.2918, Intent Accuracy: 0.2381
Position 15 - MAE: 9.9041, RMSE: 15.1189, Non-zero MAE: 34.2464, Non-zero RMSE: 48.5866, Intent Accuracy: 0.2381
Position 16 - MAE: 9.3435, RMSE: 13.9046, Non-zero MAE: 32.2893, Non-zero RMSE: 44.4879, Intent Accuracy: 0.1905
Position 17 - MAE: 9.4404, RMSE: 14.1182, Non-zero MAE: 31.6955, Non-zero RMSE: 44.4564, Intent Accuracy: 0.1905
Position 18 - MAE: 9.0943, RMSE: 13.7615, Non-zero MAE: 30.6042, Non-zero RMSE: 43.0421, Intent Accuracy: 0.1667
Position 19 - MAE: 8.8464, RMSE: 13.4350, Non-zero MAE: 29.3051, Non-zero RMSE: 41.7231, Intent Accuracy: 0.1667
Position 20 - MAE: 8.3214, RMSE: 12.7236, Non-zero MAE: 27.3080, Non-zero RMSE: 38.6478, Intent Accuracy: 0.1667
Position 21 - MAE: 8.0870, RMSE: 12.2184, Non-zero MAE: 26.4413, Non-zero RMSE: 37.3807, Intent Accuracy: 0.1667
Position 22 - MAE: 7.8457, RMSE: 11.7074, Non-zero MAE: 25.2994, Non-zero RMSE: 35.6401, Intent Accuracy: 0.1667
Position 23 - MAE: 7.5619, RMSE: 11.4748, Non-zero MAE: 24.2369, Non-zero RMSE: 35.0599, Intent Accuracy: 0.1667
Position 24 - MAE: 7.1338, RMSE: 10.6759, Non-zero MAE: 21.9534, Non-zero RMSE: 31.3634, Intent Accuracy: 0.1667
Position 25 - MAE: 7.0171, RMSE: 10.5557, Non-zero MAE: 21.4161, Non-zero RMSE: 30.7900, Intent Accuracy: 0.1667
Position 26 - MAE: 6.6901, RMSE: 10.0580, Non-zero MAE: 20.1815, Non-zero RMSE: 29.5516, Intent Accuracy: 0.1667
Position 27 - MAE: 6.4757, RMSE: 9.8410, Non-zero MAE: 18.8643, Non-zero RMSE: 27.6727, Intent Accuracy: 0.1667
Position 28 - MAE: 6.0953, RMSE: 9.1945, Non-zero MAE: 17.4549, Non-zero RMSE: 26.0546, Intent Accuracy: 0.1667
Position 29 - MAE: 5.8245, RMSE: 8.8274, Non-zero MAE: 16.2777, Non-zero RMSE: 24.2336, Intent Accuracy: 0.1667

Execution time: 3.95 seconds
Max GPU memory usage: 270.27 MB

----------------------------------------

Running test for model: tabgpt2, init_seq: 2
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=2, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabgpt2_best_model.pth', min_delta=0.001, model_type='tabgpt2', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabgpt2_best_model.pth
Overall Inference Results - MAE: 9.3610, RMSE: 24.1375, Non-zero MAE: 35.2445, Non-zero RMSE: 49.9340, Intent Accuracy: 0.1190
Position 2 - MAE: 11.8955, RMSE: 16.5091, Non-zero MAE: 37.4499, Non-zero RMSE: 47.0289, Intent Accuracy: 0.4762
Position 3 - MAE: 12.5269, RMSE: 19.3448, Non-zero MAE: 51.8497, Non-zero RMSE: 68.6039, Intent Accuracy: 0.5714
Position 4 - MAE: 12.7357, RMSE: 19.3720, Non-zero MAE: 51.2230, Non-zero RMSE: 68.8020, Intent Accuracy: 0.3333
Position 5 - MAE: 12.7246, RMSE: 19.0582, Non-zero MAE: 50.1465, Non-zero RMSE: 67.3951, Intent Accuracy: 0.1667
Position 6 - MAE: 12.2876, RMSE: 18.8768, Non-zero MAE: 47.4799, Non-zero RMSE: 64.9800, Intent Accuracy: 0.2619
Position 7 - MAE: 12.0843, RMSE: 19.0110, Non-zero MAE: 47.5024, Non-zero RMSE: 65.3765, Intent Accuracy: 0.1905
Position 8 - MAE: 11.8314, RMSE: 18.0929, Non-zero MAE: 44.2319, Non-zero RMSE: 61.1011, Intent Accuracy: 0.1667
Position 9 - MAE: 11.2733, RMSE: 17.2493, Non-zero MAE: 43.9395, Non-zero RMSE: 59.5807, Intent Accuracy: 0.0952
Position 10 - MAE: 10.9196, RMSE: 16.8933, Non-zero MAE: 42.9965, Non-zero RMSE: 59.0945, Intent Accuracy: 0.1429
Position 11 - MAE: 10.5748, RMSE: 16.2030, Non-zero MAE: 41.6897, Non-zero RMSE: 57.0215, Intent Accuracy: 0.2143
Position 12 - MAE: 10.4175, RMSE: 15.8297, Non-zero MAE: 39.5264, Non-zero RMSE: 53.5852, Intent Accuracy: 0.1429
Position 13 - MAE: 10.2474, RMSE: 15.7758, Non-zero MAE: 38.9906, Non-zero RMSE: 54.4745, Intent Accuracy: 0.1190
Position 14 - MAE: 9.8617, RMSE: 15.1937, Non-zero MAE: 37.6914, Non-zero RMSE: 51.8081, Intent Accuracy: 0.0714
Position 15 - MAE: 9.5636, RMSE: 14.8583, Non-zero MAE: 37.0010, Non-zero RMSE: 51.2250, Intent Accuracy: 0.0714
Position 16 - MAE: 9.0865, RMSE: 13.6525, Non-zero MAE: 34.8214, Non-zero RMSE: 46.9389, Intent Accuracy: 0.1667
Position 17 - MAE: 9.1313, RMSE: 13.8643, Non-zero MAE: 34.1873, Non-zero RMSE: 46.9367, Intent Accuracy: 0.1905
Position 18 - MAE: 8.6838, RMSE: 13.3915, Non-zero MAE: 32.6794, Non-zero RMSE: 45.2191, Intent Accuracy: 0.1429
Position 19 - MAE: 8.4685, RMSE: 13.0639, Non-zero MAE: 31.7190, Non-zero RMSE: 44.0680, Intent Accuracy: 0.1667
Position 20 - MAE: 7.9265, RMSE: 12.2531, Non-zero MAE: 29.4753, Non-zero RMSE: 40.7871, Intent Accuracy: 0.0714
Position 21 - MAE: 7.7314, RMSE: 11.8289, Non-zero MAE: 28.7281, Non-zero RMSE: 39.6693, Intent Accuracy: 0.1667
Position 22 - MAE: 7.4627, RMSE: 11.2858, Non-zero MAE: 27.4134, Non-zero RMSE: 37.7189, Intent Accuracy: 0.1429
Position 23 - MAE: 7.0942, RMSE: 10.9151, Non-zero MAE: 26.8630, Non-zero RMSE: 37.5435, Intent Accuracy: 0.0952
Position 24 - MAE: 6.7835, RMSE: 10.2443, Non-zero MAE: 24.1623, Non-zero RMSE: 33.5433, Intent Accuracy: 0.0476
Position 25 - MAE: 6.5800, RMSE: 10.0618, Non-zero MAE: 23.8511, Non-zero RMSE: 33.2886, Intent Accuracy: 0.1905
Position 26 - MAE: 6.2569, RMSE: 9.5339, Non-zero MAE: 22.2878, Non-zero RMSE: 31.6270, Intent Accuracy: 0.1905
Position 27 - MAE: 5.9865, RMSE: 9.2205, Non-zero MAE: 21.3662, Non-zero RMSE: 30.1661, Intent Accuracy: 0.0714
Position 28 - MAE: 5.6219, RMSE: 8.6084, Non-zero MAE: 19.7648, Non-zero RMSE: 28.2320, Intent Accuracy: 0.1190
Position 29 - MAE: 5.3589, RMSE: 8.1771, Non-zero MAE: 18.7727, Non-zero RMSE: 26.8202, Intent Accuracy: 0.1190

Execution time: 3.15 seconds
Max GPU memory usage: 325.76 MB

----------------------------------------

Running test for model: tabllama, init_seq: 2
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/default_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=2, intermediate_size=2400, learning_rate=2.491423889455553e-05, load_path='./training_results/tabllama_best_model.pth', min_delta=0.001, model_type='tabllama', noise_mean=0, noise_std=0.022400575875951165, num_epochs=300, num_heads=4, num_layers=2, patience=15, save_path='best_model.pth', seed=0, seq_num_heads=4, seq_num_layers=3, swap_prob=0.0083388429783301, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabllama_best_model.pth
Overall Inference Results - MAE: 12.6234, RMSE: 24.5586, Non-zero MAE: 26.4565, Non-zero RMSE: 41.3592, Intent Accuracy: 0.1429
Position 2 - MAE: 11.5588, RMSE: 15.7609, Non-zero MAE: 30.1188, Non-zero RMSE: 38.5304, Intent Accuracy: 0.9286
Position 3 - MAE: 13.2908, RMSE: 19.2182, Non-zero MAE: 35.0848, Non-zero RMSE: 51.3494, Intent Accuracy: 0.6429
Position 4 - MAE: 13.4272, RMSE: 19.3149, Non-zero MAE: 34.7787, Non-zero RMSE: 51.8735, Intent Accuracy: 0.5714
Position 5 - MAE: 14.6836, RMSE: 21.5307, Non-zero MAE: 37.6359, Non-zero RMSE: 55.2007, Intent Accuracy: 0.2857
Position 6 - MAE: 14.6557, RMSE: 21.8867, Non-zero MAE: 36.0542, Non-zero RMSE: 54.2361, Intent Accuracy: 0.2619
Position 7 - MAE: 14.8176, RMSE: 22.3941, Non-zero MAE: 36.0080, Non-zero RMSE: 55.1654, Intent Accuracy: 0.2143
Position 8 - MAE: 14.7462, RMSE: 22.2838, Non-zero MAE: 34.3631, Non-zero RMSE: 52.1783, Intent Accuracy: 0.1190
Position 9 - MAE: 14.2184, RMSE: 21.2327, Non-zero MAE: 33.3768, Non-zero RMSE: 50.0862, Intent Accuracy: 0.1667
Position 10 - MAE: 14.0788, RMSE: 21.4003, Non-zero MAE: 33.0969, Non-zero RMSE: 50.4523, Intent Accuracy: 0.1429
Position 11 - MAE: 13.7685, RMSE: 20.9533, Non-zero MAE: 31.6453, Non-zero RMSE: 48.5375, Intent Accuracy: 0.0952
Position 12 - MAE: 13.5319, RMSE: 20.5281, Non-zero MAE: 30.0294, Non-zero RMSE: 44.9419, Intent Accuracy: 0.1667
Position 13 - MAE: 13.5024, RMSE: 20.6913, Non-zero MAE: 29.9674, Non-zero RMSE: 46.5753, Intent Accuracy: 0.1190
Position 14 - MAE: 13.1304, RMSE: 20.0828, Non-zero MAE: 28.4367, Non-zero RMSE: 42.9825, Intent Accuracy: 0.2143
Position 15 - MAE: 13.0871, RMSE: 20.2265, Non-zero MAE: 27.4683, Non-zero RMSE: 42.7350, Intent Accuracy: 0.1429
Position 16 - MAE: 12.7740, RMSE: 19.7029, Non-zero MAE: 25.9365, Non-zero RMSE: 38.9154, Intent Accuracy: 0.1429
Position 17 - MAE: 12.8050, RMSE: 19.9712, Non-zero MAE: 25.9616, Non-zero RMSE: 39.6236, Intent Accuracy: 0.1190
Position 18 - MAE: 12.6730, RMSE: 19.7867, Non-zero MAE: 24.5937, Non-zero RMSE: 37.8395, Intent Accuracy: 0.1190
Position 19 - MAE: 12.3784, RMSE: 19.2885, Non-zero MAE: 23.6773, Non-zero RMSE: 36.9218, Intent Accuracy: 0.1429
Position 20 - MAE: 11.9463, RMSE: 18.6646, Non-zero MAE: 22.0115, Non-zero RMSE: 33.9325, Intent Accuracy: 0.1429
Position 21 - MAE: 11.5523, RMSE: 18.0128, Non-zero MAE: 21.1648, Non-zero RMSE: 32.8526, Intent Accuracy: 0.1667
Position 22 - MAE: 11.5094, RMSE: 17.7893, Non-zero MAE: 20.1314, Non-zero RMSE: 30.9215, Intent Accuracy: 0.1190
Position 23 - MAE: 11.3794, RMSE: 17.9067, Non-zero MAE: 19.7614, Non-zero RMSE: 31.1674, Intent Accuracy: 0.1429
Position 24 - MAE: 10.8964, RMSE: 17.2800, Non-zero MAE: 17.9313, Non-zero RMSE: 27.7651, Intent Accuracy: 0.1429
Position 25 - MAE: 10.8151, RMSE: 17.3363, Non-zero MAE: 17.7701, Non-zero RMSE: 27.9813, Intent Accuracy: 0.1905
Position 26 - MAE: 10.6309, RMSE: 17.1542, Non-zero MAE: 16.9855, Non-zero RMSE: 27.0611, Intent Accuracy: 0.1429
Position 27 - MAE: 10.4991, RMSE: 16.8868, Non-zero MAE: 16.3611, Non-zero RMSE: 25.9425, Intent Accuracy: 0.1429
Position 28 - MAE: 10.4025, RMSE: 16.3814, Non-zero MAE: 15.7104, Non-zero RMSE: 24.7584, Intent Accuracy: 0.1190
Position 29 - MAE: 10.3066, RMSE: 16.4409, Non-zero MAE: 15.1782, Non-zero RMSE: 23.5172, Intent Accuracy: 0.1429

Execution time: 3.73 seconds
Max GPU memory usage: 337.40 MB

----------------------------------------

Running test for model: rnn, init_seq: 3
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=3, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/rnn_best_model.pth', min_delta=0.001, model_type='rnn', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/rnn_best_model.pth
Overall Inference Results - MAE: 12.5767, RMSE: 33.9316, Non-zero MAE: 39.9178, Non-zero RMSE: 56.1550, Intent Accuracy: 0.1667
Position 3 - MAE: 12.9299, RMSE: 19.2692, Non-zero MAE: 42.2111, Non-zero RMSE: 50.9470, Intent Accuracy: 0.1429
Position 4 - MAE: 13.7622, RMSE: 22.0556, Non-zero MAE: 50.5918, Non-zero RMSE: 67.5193, Intent Accuracy: 0.1429
Position 5 - MAE: 13.1729, RMSE: 20.7110, Non-zero MAE: 48.9106, Non-zero RMSE: 65.1648, Intent Accuracy: 0.1429
Position 6 - MAE: 12.8289, RMSE: 20.4370, Non-zero MAE: 45.6649, Non-zero RMSE: 60.3934, Intent Accuracy: 0.3810
Position 7 - MAE: 13.3708, RMSE: 21.9248, Non-zero MAE: 49.3977, Non-zero RMSE: 65.7611, Intent Accuracy: 0.1429
Position 8 - MAE: 13.1377, RMSE: 22.0558, Non-zero MAE: 48.9081, Non-zero RMSE: 65.5305, Intent Accuracy: 0.1429
Position 9 - MAE: 13.8564, RMSE: 23.2112, Non-zero MAE: 47.4601, Non-zero RMSE: 63.9775, Intent Accuracy: 0.1667
Position 10 - MAE: 12.2410, RMSE: 19.6298, Non-zero MAE: 44.5809, Non-zero RMSE: 59.6012, Intent Accuracy: 0.1667
Position 11 - MAE: 13.0978, RMSE: 22.9437, Non-zero MAE: 45.0622, Non-zero RMSE: 61.1339, Intent Accuracy: 0.1667
Position 12 - MAE: 13.0310, RMSE: 22.9536, Non-zero MAE: 43.8356, Non-zero RMSE: 59.1013, Intent Accuracy: 0.1667
Position 13 - MAE: 12.6338, RMSE: 22.0625, Non-zero MAE: 42.6790, Non-zero RMSE: 56.8934, Intent Accuracy: 0.1905
Position 14 - MAE: 13.0354, RMSE: 24.6382, Non-zero MAE: 43.0513, Non-zero RMSE: 59.0262, Intent Accuracy: 0.1905
Position 15 - MAE: 13.0822, RMSE: 25.7184, Non-zero MAE: 42.1896, Non-zero RMSE: 57.3251, Intent Accuracy: 0.1667
Position 16 - MAE: 13.3865, RMSE: 27.1686, Non-zero MAE: 41.9692, Non-zero RMSE: 59.2348, Intent Accuracy: 0.1905
Position 17 - MAE: 13.2823, RMSE: 28.1596, Non-zero MAE: 40.9651, Non-zero RMSE: 57.0559, Intent Accuracy: 0.1429
Position 18 - MAE: 13.4550, RMSE: 28.3747, Non-zero MAE: 39.6671, Non-zero RMSE: 56.3575, Intent Accuracy: 0.1429
Position 19 - MAE: 12.6696, RMSE: 27.1519, Non-zero MAE: 37.2742, Non-zero RMSE: 51.8228, Intent Accuracy: 0.1905
Position 20 - MAE: 12.0134, RMSE: 24.9573, Non-zero MAE: 36.0449, Non-zero RMSE: 50.2281, Intent Accuracy: 0.1905
Position 21 - MAE: 12.2113, RMSE: 27.4784, Non-zero MAE: 35.9564, Non-zero RMSE: 52.1372, Intent Accuracy: 0.2143
Position 22 - MAE: 12.6804, RMSE: 29.3737, Non-zero MAE: 35.3245, Non-zero RMSE: 51.8810, Intent Accuracy: 0.1667
Position 23 - MAE: 12.4521, RMSE: 29.6256, Non-zero MAE: 34.5748, Non-zero RMSE: 51.0225, Intent Accuracy: 0.1429
Position 24 - MAE: 12.1449, RMSE: 28.5804, Non-zero MAE: 34.5224, Non-zero RMSE: 53.4767, Intent Accuracy: 0.2143
Position 25 - MAE: 11.3165, RMSE: 26.9887, Non-zero MAE: 31.0637, Non-zero RMSE: 46.6108, Intent Accuracy: 0.1905
Position 26 - MAE: 11.0842, RMSE: 26.6155, Non-zero MAE: 30.6387, Non-zero RMSE: 47.5096, Intent Accuracy: 0.1667
Position 27 - MAE: 10.7716, RMSE: 25.9314, Non-zero MAE: 29.0284, Non-zero RMSE: 44.9839, Intent Accuracy: 0.1667
Position 28 - MAE: 11.1727, RMSE: 28.9040, Non-zero MAE: 28.3834, Non-zero RMSE: 45.1613, Intent Accuracy: 0.1667
Position 29 - MAE: 11.0314, RMSE: 28.7364, Non-zero MAE: 28.3399, Non-zero RMSE: 46.9453, Intent Accuracy: 0.1667

Execution time: 3.14 seconds
Max GPU memory usage: 21.75 MB

----------------------------------------

Running test for model: lstm, init_seq: 3
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=3, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/lstm_best_model.pth', min_delta=0.001, model_type='lstm', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/lstm_best_model.pth
Overall Inference Results - MAE: 9.1514, RMSE: 25.3956, Non-zero MAE: 39.1038, Non-zero RMSE: 52.9533, Intent Accuracy: 0.1429
Position 3 - MAE: 11.8297, RMSE: 16.9502, Non-zero MAE: 46.8134, Non-zero RMSE: 55.2041, Intent Accuracy: 0.2619
Position 4 - MAE: 12.5522, RMSE: 19.6311, Non-zero MAE: 53.7621, Non-zero RMSE: 70.6836, Intent Accuracy: 0.2857
Position 5 - MAE: 12.3361, RMSE: 19.0402, Non-zero MAE: 52.2794, Non-zero RMSE: 69.0061, Intent Accuracy: 0.2857
Position 6 - MAE: 11.7086, RMSE: 17.9363, Non-zero MAE: 51.1992, Non-zero RMSE: 66.6694, Intent Accuracy: 0.6190
Position 7 - MAE: 11.8615, RMSE: 18.6982, Non-zero MAE: 51.9346, Non-zero RMSE: 68.5085, Intent Accuracy: 0.0238
Position 8 - MAE: 11.6840, RMSE: 18.9203, Non-zero MAE: 51.2256, Non-zero RMSE: 68.3342, Intent Accuracy: 0.0238
Position 9 - MAE: 11.4536, RMSE: 17.9953, Non-zero MAE: 48.3862, Non-zero RMSE: 64.3825, Intent Accuracy: 0.0000
Position 10 - MAE: 10.9019, RMSE: 17.0126, Non-zero MAE: 47.2548, Non-zero RMSE: 62.1250, Intent Accuracy: 0.0476
Position 11 - MAE: 10.5888, RMSE: 16.7234, Non-zero MAE: 46.1650, Non-zero RMSE: 61.4770, Intent Accuracy: 0.0952
Position 12 - MAE: 10.2623, RMSE: 16.0300, Non-zero MAE: 44.6846, Non-zero RMSE: 59.3015, Intent Accuracy: 0.0952
Position 13 - MAE: 10.1124, RMSE: 15.8206, Non-zero MAE: 43.1540, Non-zero RMSE: 56.3868, Intent Accuracy: 0.1190
Position 14 - MAE: 9.8570, RMSE: 15.5367, Non-zero MAE: 42.5927, Non-zero RMSE: 57.1320, Intent Accuracy: 0.2143
Position 15 - MAE: 9.4422, RMSE: 14.9255, Non-zero MAE: 41.0727, Non-zero RMSE: 54.2784, Intent Accuracy: 0.1429
Position 16 - MAE: 9.2371, RMSE: 14.6944, Non-zero MAE: 39.7858, Non-zero RMSE: 53.3573, Intent Accuracy: 0.1429
Position 17 - MAE: 8.7445, RMSE: 13.5054, Non-zero MAE: 37.8374, Non-zero RMSE: 49.2806, Intent Accuracy: 0.1667
Position 18 - MAE: 8.8695, RMSE: 13.7701, Non-zero MAE: 37.1411, Non-zero RMSE: 49.1277, Intent Accuracy: 0.1905
Position 19 - MAE: 8.3505, RMSE: 13.1633, Non-zero MAE: 35.9993, Non-zero RMSE: 47.7095, Intent Accuracy: 0.1190
Position 20 - MAE: 8.1315, RMSE: 12.8997, Non-zero MAE: 34.7201, Non-zero RMSE: 46.3069, Intent Accuracy: 0.1667
Position 21 - MAE: 7.6013, RMSE: 12.1675, Non-zero MAE: 32.8042, Non-zero RMSE: 43.2881, Intent Accuracy: 0.1429
Position 22 - MAE: 7.3739, RMSE: 11.6244, Non-zero MAE: 31.8296, Non-zero RMSE: 42.0296, Intent Accuracy: 0.1429
Position 23 - MAE: 7.1166, RMSE: 11.0816, Non-zero MAE: 30.6103, Non-zero RMSE: 40.1187, Intent Accuracy: 0.1429
Position 24 - MAE: 6.7820, RMSE: 10.7301, Non-zero MAE: 29.3980, Non-zero RMSE: 39.4302, Intent Accuracy: 0.0952
Position 25 - MAE: 6.4738, RMSE: 10.0702, Non-zero MAE: 27.0556, Non-zero RMSE: 35.7131, Intent Accuracy: 0.1429
Position 26 - MAE: 6.2551, RMSE: 9.8534, Non-zero MAE: 26.5519, Non-zero RMSE: 35.2013, Intent Accuracy: 0.1429
Position 27 - MAE: 5.8761, RMSE: 9.2079, Non-zero MAE: 25.3346, Non-zero RMSE: 33.8836, Intent Accuracy: 0.1190
Position 28 - MAE: 5.6400, RMSE: 8.9335, Non-zero MAE: 24.0260, Non-zero RMSE: 32.0541, Intent Accuracy: 0.1429
Position 29 - MAE: 5.2939, RMSE: 8.4104, Non-zero MAE: 22.5575, Non-zero RMSE: 30.2974, Intent Accuracy: 0.1429

Execution time: 3.83 seconds
Max GPU memory usage: 37.83 MB

----------------------------------------

Running test for model: vanillatf, init_seq: 3
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=3, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/vanillatf_best_model.pth', min_delta=0.001, model_type='vanillatf', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/vanillatf_best_model.pth
Overall Inference Results - MAE: 9.4062, RMSE: 22.7593, Non-zero MAE: 32.9693, Non-zero RMSE: 46.3812, Intent Accuracy: 0.1429
Position 3 - MAE: 11.7284, RMSE: 15.7443, Non-zero MAE: 26.7482, Non-zero RMSE: 35.5363, Intent Accuracy: 0.9048
Position 4 - MAE: 12.1803, RMSE: 18.0877, Non-zero MAE: 42.2088, Non-zero RMSE: 57.9431, Intent Accuracy: 0.8095
Position 5 - MAE: 12.0680, RMSE: 17.9697, Non-zero MAE: 46.6106, Non-zero RMSE: 62.2663, Intent Accuracy: 0.6190
Position 6 - MAE: 11.6528, RMSE: 17.0696, Non-zero MAE: 44.4480, Non-zero RMSE: 59.1736, Intent Accuracy: 0.6905
Position 7 - MAE: 11.2105, RMSE: 16.6575, Non-zero MAE: 41.6618, Non-zero RMSE: 56.1808, Intent Accuracy: 0.7857
Position 8 - MAE: 11.3007, RMSE: 17.3315, Non-zero MAE: 44.1755, Non-zero RMSE: 60.0182, Intent Accuracy: 0.5238
Position 9 - MAE: 10.9926, RMSE: 16.3521, Non-zero MAE: 41.5259, Non-zero RMSE: 56.2178, Intent Accuracy: 0.5000
Position 10 - MAE: 10.4034, RMSE: 15.3884, Non-zero MAE: 40.2915, Non-zero RMSE: 53.8547, Intent Accuracy: 0.5238
Position 11 - MAE: 10.2179, RMSE: 15.3720, Non-zero MAE: 40.2779, Non-zero RMSE: 54.4931, Intent Accuracy: 0.4048
Position 12 - MAE: 10.0291, RMSE: 15.0826, Non-zero MAE: 39.9319, Non-zero RMSE: 54.1720, Intent Accuracy: 0.4286
Position 13 - MAE: 9.9801, RMSE: 14.9693, Non-zero MAE: 38.1297, Non-zero RMSE: 51.0773, Intent Accuracy: 0.4048
Position 14 - MAE: 9.7662, RMSE: 14.8240, Non-zero MAE: 37.6554, Non-zero RMSE: 51.7776, Intent Accuracy: 0.3810
Position 15 - MAE: 9.5276, RMSE: 14.4237, Non-zero MAE: 36.5543, Non-zero RMSE: 49.4965, Intent Accuracy: 0.3333
Position 16 - MAE: 9.5552, RMSE: 14.5352, Non-zero MAE: 34.5851, Non-zero RMSE: 48.5614, Intent Accuracy: 0.2857
Position 17 - MAE: 8.8744, RMSE: 13.2555, Non-zero MAE: 33.1581, Non-zero RMSE: 44.7688, Intent Accuracy: 0.1905
Position 18 - MAE: 9.0135, RMSE: 13.5017, Non-zero MAE: 32.2634, Non-zero RMSE: 44.4998, Intent Accuracy: 0.2857
Position 19 - MAE: 8.6255, RMSE: 13.1242, Non-zero MAE: 31.4710, Non-zero RMSE: 43.2541, Intent Accuracy: 0.1667
Position 20 - MAE: 8.6708, RMSE: 13.1197, Non-zero MAE: 29.2827, Non-zero RMSE: 41.2336, Intent Accuracy: 0.1667
Position 21 - MAE: 8.2628, RMSE: 12.5629, Non-zero MAE: 27.5303, Non-zero RMSE: 38.4729, Intent Accuracy: 0.1905
Position 22 - MAE: 7.9670, RMSE: 12.0701, Non-zero MAE: 27.3155, Non-zero RMSE: 37.9100, Intent Accuracy: 0.1429
Position 23 - MAE: 7.7317, RMSE: 11.5860, Non-zero MAE: 26.0157, Non-zero RMSE: 35.7143, Intent Accuracy: 0.1905
Position 24 - MAE: 7.4098, RMSE: 11.2763, Non-zero MAE: 25.2316, Non-zero RMSE: 35.5499, Intent Accuracy: 0.1667
Position 25 - MAE: 7.1027, RMSE: 10.7579, Non-zero MAE: 22.7958, Non-zero RMSE: 31.7040, Intent Accuracy: 0.1667
Position 26 - MAE: 7.0312, RMSE: 10.7185, Non-zero MAE: 22.2685, Non-zero RMSE: 31.5055, Intent Accuracy: 0.1429
Position 27 - MAE: 6.6805, RMSE: 10.2156, Non-zero MAE: 21.2276, Non-zero RMSE: 30.1777, Intent Accuracy: 0.1429
Position 28 - MAE: 6.4826, RMSE: 10.0290, Non-zero MAE: 19.7677, Non-zero RMSE: 28.3291, Intent Accuracy: 0.1429
Position 29 - MAE: 6.2249, RMSE: 9.7493, Non-zero MAE: 18.4576, Non-zero RMSE: 26.7438, Intent Accuracy: 0.1429

Execution time: 3.09 seconds
Max GPU memory usage: 123.11 MB

----------------------------------------

Running test for model: tabbert, init_seq: 3
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=3, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabbert_best_model.pth', min_delta=0.001, model_type='tabbert', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabbert_best_model.pth
Overall Inference Results - MAE: 9.7439, RMSE: 23.3587, Non-zero MAE: 33.1076, Non-zero RMSE: 47.7616, Intent Accuracy: 0.2143
Position 3 - MAE: 11.6169, RMSE: 15.7882, Non-zero MAE: 29.5613, Non-zero RMSE: 38.2466, Intent Accuracy: 0.9762
Position 4 - MAE: 12.0930, RMSE: 18.4596, Non-zero MAE: 45.3430, Non-zero RMSE: 61.8419, Intent Accuracy: 0.7381
Position 5 - MAE: 12.7831, RMSE: 19.3898, Non-zero MAE: 48.1548, Non-zero RMSE: 65.6299, Intent Accuracy: 0.5238
Position 6 - MAE: 12.7620, RMSE: 19.1556, Non-zero MAE: 48.3418, Non-zero RMSE: 65.2686, Intent Accuracy: 0.3571
Position 7 - MAE: 12.4107, RMSE: 18.9304, Non-zero MAE: 45.9902, Non-zero RMSE: 63.0414, Intent Accuracy: 0.3810
Position 8 - MAE: 12.2542, RMSE: 19.0323, Non-zero MAE: 45.2566, Non-zero RMSE: 63.0844, Intent Accuracy: 0.3571
Position 9 - MAE: 11.9294, RMSE: 18.1260, Non-zero MAE: 42.5170, Non-zero RMSE: 59.1238, Intent Accuracy: 0.3571
Position 10 - MAE: 11.4322, RMSE: 17.2160, Non-zero MAE: 41.4705, Non-zero RMSE: 56.8459, Intent Accuracy: 0.3571
Position 11 - MAE: 11.0782, RMSE: 16.8785, Non-zero MAE: 40.3550, Non-zero RMSE: 56.3822, Intent Accuracy: 0.3810
Position 12 - MAE: 10.8187, RMSE: 16.3521, Non-zero MAE: 38.8541, Non-zero RMSE: 54.2354, Intent Accuracy: 0.3571
Position 13 - MAE: 10.6644, RMSE: 16.0204, Non-zero MAE: 37.3891, Non-zero RMSE: 51.1765, Intent Accuracy: 0.3571
Position 14 - MAE: 10.4499, RMSE: 15.8789, Non-zero MAE: 36.8130, Non-zero RMSE: 52.0713, Intent Accuracy: 0.3571
Position 15 - MAE: 10.0562, RMSE: 15.2515, Non-zero MAE: 35.3093, Non-zero RMSE: 49.1550, Intent Accuracy: 0.2857
Position 16 - MAE: 9.8720, RMSE: 15.0619, Non-zero MAE: 34.1515, Non-zero RMSE: 48.4196, Intent Accuracy: 0.2857
Position 17 - MAE: 9.3291, RMSE: 13.8777, Non-zero MAE: 32.2205, Non-zero RMSE: 44.3779, Intent Accuracy: 0.2857
Position 18 - MAE: 9.4047, RMSE: 14.0578, Non-zero MAE: 31.6041, Non-zero RMSE: 44.3338, Intent Accuracy: 0.2619
Position 19 - MAE: 9.0403, RMSE: 13.6597, Non-zero MAE: 30.4489, Non-zero RMSE: 42.8129, Intent Accuracy: 0.2619
Position 20 - MAE: 8.8028, RMSE: 13.3481, Non-zero MAE: 29.1957, Non-zero RMSE: 41.4856, Intent Accuracy: 0.2619
Position 21 - MAE: 8.2768, RMSE: 12.6383, Non-zero MAE: 27.1829, Non-zero RMSE: 38.4793, Intent Accuracy: 0.2619
Position 22 - MAE: 8.0323, RMSE: 12.1113, Non-zero MAE: 26.3027, Non-zero RMSE: 37.1613, Intent Accuracy: 0.2619
Position 23 - MAE: 7.7931, RMSE: 11.6100, Non-zero MAE: 25.1941, Non-zero RMSE: 35.4436, Intent Accuracy: 0.2143
Position 24 - MAE: 7.5180, RMSE: 11.3900, Non-zero MAE: 24.0929, Non-zero RMSE: 34.8607, Intent Accuracy: 0.2143
Position 25 - MAE: 7.1097, RMSE: 10.6201, Non-zero MAE: 21.8656, Non-zero RMSE: 31.2590, Intent Accuracy: 0.2143
Position 26 - MAE: 6.9944, RMSE: 10.5051, Non-zero MAE: 21.3613, Non-zero RMSE: 30.6948, Intent Accuracy: 0.2143
Position 27 - MAE: 6.6421, RMSE: 9.9715, Non-zero MAE: 20.0904, Non-zero RMSE: 29.4527, Intent Accuracy: 0.2143
Position 28 - MAE: 6.4425, RMSE: 9.7765, Non-zero MAE: 18.8044, Non-zero RMSE: 27.5598, Intent Accuracy: 0.2143
Position 29 - MAE: 6.0771, RMSE: 9.1459, Non-zero MAE: 17.4143, Non-zero RMSE: 26.0057, Intent Accuracy: 0.2143

Execution time: 3.47 seconds
Max GPU memory usage: 270.45 MB

----------------------------------------

Running test for model: tabgpt2, init_seq: 3
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=3, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabgpt2_best_model.pth', min_delta=0.001, model_type='tabgpt2', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabgpt2_best_model.pth
Overall Inference Results - MAE: 9.5125, RMSE: 24.4259, Non-zero MAE: 35.7817, Non-zero RMSE: 50.5440, Intent Accuracy: 0.1190
Position 3 - MAE: 12.0045, RMSE: 16.6685, Non-zero MAE: 37.9809, Non-zero RMSE: 47.5599, Intent Accuracy: 0.4048
Position 4 - MAE: 12.5549, RMSE: 19.4170, Non-zero MAE: 51.8612, Non-zero RMSE: 68.6726, Intent Accuracy: 0.6429
Position 5 - MAE: 12.6285, RMSE: 19.3968, Non-zero MAE: 51.9495, Non-zero RMSE: 69.3739, Intent Accuracy: 0.3571
Position 6 - MAE: 12.7095, RMSE: 18.9297, Non-zero MAE: 50.4122, Non-zero RMSE: 67.6783, Intent Accuracy: 0.3095
Position 7 - MAE: 12.2874, RMSE: 18.7563, Non-zero MAE: 47.6460, Non-zero RMSE: 65.0489, Intent Accuracy: 0.1429
Position 8 - MAE: 12.1909, RMSE: 19.0925, Non-zero MAE: 46.3632, Non-zero RMSE: 64.5246, Intent Accuracy: 0.2857
Position 9 - MAE: 11.7669, RMSE: 18.1370, Non-zero MAE: 44.4710, Non-zero RMSE: 61.3625, Intent Accuracy: 0.1190
Position 10 - MAE: 11.2076, RMSE: 17.1486, Non-zero MAE: 44.0098, Non-zero RMSE: 59.5704, Intent Accuracy: 0.0952
Position 11 - MAE: 10.9124, RMSE: 16.8252, Non-zero MAE: 42.9871, Non-zero RMSE: 58.9288, Intent Accuracy: 0.1429
Position 12 - MAE: 10.5942, RMSE: 16.2500, Non-zero MAE: 41.6020, Non-zero RMSE: 56.8720, Intent Accuracy: 0.1190
Position 13 - MAE: 10.5284, RMSE: 16.0158, Non-zero MAE: 39.5051, Non-zero RMSE: 53.6772, Intent Accuracy: 0.1905
Position 14 - MAE: 10.2319, RMSE: 15.7250, Non-zero MAE: 38.9543, Non-zero RMSE: 54.4154, Intent Accuracy: 0.1190
Position 15 - MAE: 9.8059, RMSE: 15.1250, Non-zero MAE: 37.8431, Non-zero RMSE: 51.8301, Intent Accuracy: 0.0714
Position 16 - MAE: 9.5152, RMSE: 14.7714, Non-zero MAE: 36.6562, Non-zero RMSE: 51.0206, Intent Accuracy: 0.0714
Position 17 - MAE: 8.9951, RMSE: 13.5660, Non-zero MAE: 34.5928, Non-zero RMSE: 46.7239, Intent Accuracy: 0.1190
Position 18 - MAE: 9.1952, RMSE: 13.9127, Non-zero MAE: 33.5866, Non-zero RMSE: 46.5244, Intent Accuracy: 0.1667
Position 19 - MAE: 8.7190, RMSE: 13.3184, Non-zero MAE: 32.5353, Non-zero RMSE: 45.0620, Intent Accuracy: 0.1190
Position 20 - MAE: 8.5004, RMSE: 13.0938, Non-zero MAE: 31.5454, Non-zero RMSE: 43.9007, Intent Accuracy: 0.1190
Position 21 - MAE: 7.9194, RMSE: 12.3161, Non-zero MAE: 29.6192, Non-zero RMSE: 40.7903, Intent Accuracy: 0.1190
Position 22 - MAE: 7.6879, RMSE: 11.7873, Non-zero MAE: 28.5940, Non-zero RMSE: 39.5940, Intent Accuracy: 0.1429
Position 23 - MAE: 7.4871, RMSE: 11.2169, Non-zero MAE: 27.2558, Non-zero RMSE: 37.5833, Intent Accuracy: 0.2143
Position 24 - MAE: 7.1381, RMSE: 10.9169, Non-zero MAE: 26.2200, Non-zero RMSE: 37.0565, Intent Accuracy: 0.1429
Position 25 - MAE: 6.8072, RMSE: 10.2797, Non-zero MAE: 24.2508, Non-zero RMSE: 33.6262, Intent Accuracy: 0.0714
Position 26 - MAE: 6.5627, RMSE: 9.9493, Non-zero MAE: 23.7811, Non-zero RMSE: 33.0817, Intent Accuracy: 0.0238
Position 27 - MAE: 6.1996, RMSE: 9.3899, Non-zero MAE: 22.3668, Non-zero RMSE: 31.6295, Intent Accuracy: 0.0952
Position 28 - MAE: 6.0997, RMSE: 9.2854, Non-zero MAE: 20.8893, Non-zero RMSE: 29.7678, Intent Accuracy: 0.1905
Position 29 - MAE: 5.6379, RMSE: 8.5926, Non-zero MAE: 19.5448, Non-zero RMSE: 28.1257, Intent Accuracy: 0.1190

Execution time: 4.36 seconds
Max GPU memory usage: 325.95 MB

----------------------------------------

Running test for model: tabllama, init_seq: 3
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/default_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=3, intermediate_size=2400, learning_rate=2.491423889455553e-05, load_path='./training_results/tabllama_best_model.pth', min_delta=0.001, model_type='tabllama', noise_mean=0, noise_std=0.022400575875951165, num_epochs=300, num_heads=4, num_layers=2, patience=15, save_path='best_model.pth', seed=0, seq_num_heads=4, seq_num_layers=3, swap_prob=0.0083388429783301, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabllama_best_model.pth
Overall Inference Results - MAE: 12.6640, RMSE: 24.6798, Non-zero MAE: 26.6719, Non-zero RMSE: 41.5652, Intent Accuracy: 0.1429
Position 3 - MAE: 11.5761, RMSE: 15.7804, Non-zero MAE: 30.0589, Non-zero RMSE: 38.4827, Intent Accuracy: 0.9286
Position 4 - MAE: 13.1758, RMSE: 18.9676, Non-zero MAE: 34.0722, Non-zero RMSE: 50.1810, Intent Accuracy: 0.7381
Position 5 - MAE: 13.6184, RMSE: 19.5212, Non-zero MAE: 35.4743, Non-zero RMSE: 52.5411, Intent Accuracy: 0.5952
Position 6 - MAE: 13.6102, RMSE: 19.6198, Non-zero MAE: 35.2289, Non-zero RMSE: 52.0456, Intent Accuracy: 0.4762
Position 7 - MAE: 14.2479, RMSE: 21.2092, Non-zero MAE: 35.9157, Non-zero RMSE: 54.0633, Intent Accuracy: 0.2857
Position 8 - MAE: 14.4163, RMSE: 21.7009, Non-zero MAE: 35.8391, Non-zero RMSE: 54.7828, Intent Accuracy: 0.2381
Position 9 - MAE: 14.3479, RMSE: 21.5408, Non-zero MAE: 33.6890, Non-zero RMSE: 51.4572, Intent Accuracy: 0.2381
Position 10 - MAE: 14.2497, RMSE: 21.2441, Non-zero MAE: 33.3135, Non-zero RMSE: 50.0902, Intent Accuracy: 0.2381
Position 11 - MAE: 14.0613, RMSE: 21.1052, Non-zero MAE: 32.5467, Non-zero RMSE: 49.7829, Intent Accuracy: 0.1429
Position 12 - MAE: 13.6643, RMSE: 20.4435, Non-zero MAE: 31.1269, Non-zero RMSE: 47.5763, Intent Accuracy: 0.2143
Position 13 - MAE: 13.5416, RMSE: 20.4323, Non-zero MAE: 29.5216, Non-zero RMSE: 44.4746, Intent Accuracy: 0.2143
Position 14 - MAE: 13.2237, RMSE: 20.0553, Non-zero MAE: 29.1396, Non-zero RMSE: 45.5566, Intent Accuracy: 0.2143
Position 15 - MAE: 13.1545, RMSE: 20.1248, Non-zero MAE: 28.5090, Non-zero RMSE: 43.4943, Intent Accuracy: 0.1667
Position 16 - MAE: 13.0826, RMSE: 20.1079, Non-zero MAE: 27.3081, Non-zero RMSE: 42.4972, Intent Accuracy: 0.1429
Position 17 - MAE: 12.6093, RMSE: 19.0996, Non-zero MAE: 25.7889, Non-zero RMSE: 39.1250, Intent Accuracy: 0.1667
Position 18 - MAE: 12.7119, RMSE: 19.6212, Non-zero MAE: 25.8226, Non-zero RMSE: 39.4200, Intent Accuracy: 0.1905
Position 19 - MAE: 12.6308, RMSE: 19.6962, Non-zero MAE: 24.6137, Non-zero RMSE: 37.7492, Intent Accuracy: 0.1667
Position 20 - MAE: 12.5234, RMSE: 19.7846, Non-zero MAE: 23.9168, Non-zero RMSE: 37.3217, Intent Accuracy: 0.1667
Position 21 - MAE: 12.0789, RMSE: 19.4201, Non-zero MAE: 22.0714, Non-zero RMSE: 34.3613, Intent Accuracy: 0.1905
Position 22 - MAE: 11.9647, RMSE: 18.7239, Non-zero MAE: 21.5743, Non-zero RMSE: 33.3662, Intent Accuracy: 0.1190
Position 23 - MAE: 11.6703, RMSE: 18.2657, Non-zero MAE: 20.1948, Non-zero RMSE: 31.2790, Intent Accuracy: 0.1429
Position 24 - MAE: 11.4301, RMSE: 18.1255, Non-zero MAE: 19.6163, Non-zero RMSE: 30.9356, Intent Accuracy: 0.1667
Position 25 - MAE: 11.1355, RMSE: 17.8780, Non-zero MAE: 18.0791, Non-zero RMSE: 28.1324, Intent Accuracy: 0.1667
Position 26 - MAE: 11.1839, RMSE: 17.9628, Non-zero MAE: 17.9962, Non-zero RMSE: 27.8902, Intent Accuracy: 0.1429
Position 27 - MAE: 11.0105, RMSE: 17.7684, Non-zero MAE: 17.1323, Non-zero RMSE: 26.9579, Intent Accuracy: 0.1429
Position 28 - MAE: 10.7375, RMSE: 17.3300, Non-zero MAE: 16.2521, Non-zero RMSE: 25.6136, Intent Accuracy: 0.1190
Position 29 - MAE: 10.6716, RMSE: 17.2888, Non-zero MAE: 15.7383, Non-zero RMSE: 24.8751, Intent Accuracy: 0.1429

Execution time: 3.63 seconds
Max GPU memory usage: 337.58 MB

----------------------------------------

Running test for model: rnn, init_seq: 4
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/rnn_best_model.pth', min_delta=0.001, model_type='rnn', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/rnn_best_model.pth
Overall Inference Results - MAE: 12.3768, RMSE: 31.4403, Non-zero MAE: 39.4688, Non-zero RMSE: 54.1566, Intent Accuracy: 0.1429
Position 4 - MAE: 13.4139, RMSE: 20.6939, Non-zero MAE: 41.3470, Non-zero RMSE: 50.3448, Intent Accuracy: 0.1429
Position 5 - MAE: 14.0328, RMSE: 22.5340, Non-zero MAE: 50.4573, Non-zero RMSE: 67.4745, Intent Accuracy: 0.1429
Position 6 - MAE: 13.7080, RMSE: 21.9723, Non-zero MAE: 51.5908, Non-zero RMSE: 68.4842, Intent Accuracy: 0.1429
Position 7 - MAE: 13.1652, RMSE: 20.8630, Non-zero MAE: 49.9848, Non-zero RMSE: 66.3750, Intent Accuracy: 0.1429
Position 8 - MAE: 12.2676, RMSE: 19.1656, Non-zero MAE: 45.9962, Non-zero RMSE: 61.1531, Intent Accuracy: 0.1905
Position 9 - MAE: 12.8572, RMSE: 21.0717, Non-zero MAE: 48.2455, Non-zero RMSE: 65.1428, Intent Accuracy: 0.1429
Position 10 - MAE: 12.7244, RMSE: 20.6656, Non-zero MAE: 46.3839, Non-zero RMSE: 61.7430, Intent Accuracy: 0.1429
Position 11 - MAE: 12.8773, RMSE: 21.5178, Non-zero MAE: 45.4649, Non-zero RMSE: 60.2179, Intent Accuracy: 0.1429
Position 12 - MAE: 12.4405, RMSE: 20.7883, Non-zero MAE: 43.8954, Non-zero RMSE: 59.3624, Intent Accuracy: 0.1429
Position 13 - MAE: 12.0154, RMSE: 19.8429, Non-zero MAE: 42.2030, Non-zero RMSE: 56.9573, Intent Accuracy: 0.1429
Position 14 - MAE: 12.1956, RMSE: 21.0369, Non-zero MAE: 40.6762, Non-zero RMSE: 53.8731, Intent Accuracy: 0.1429
Position 15 - MAE: 13.0202, RMSE: 23.8909, Non-zero MAE: 41.9993, Non-zero RMSE: 57.3815, Intent Accuracy: 0.1429
Position 16 - MAE: 12.4451, RMSE: 23.0188, Non-zero MAE: 39.4384, Non-zero RMSE: 52.8717, Intent Accuracy: 0.1429
Position 17 - MAE: 11.9870, RMSE: 22.2890, Non-zero MAE: 39.5262, Non-zero RMSE: 53.3425, Intent Accuracy: 0.1429
Position 18 - MAE: 11.7020, RMSE: 22.0920, Non-zero MAE: 38.2107, Non-zero RMSE: 50.7109, Intent Accuracy: 0.1429
Position 19 - MAE: 12.2657, RMSE: 23.8324, Non-zero MAE: 38.0538, Non-zero RMSE: 51.1509, Intent Accuracy: 0.1429
Position 20 - MAE: 11.7205, RMSE: 23.4166, Non-zero MAE: 36.7230, Non-zero RMSE: 49.8602, Intent Accuracy: 0.1429
Position 21 - MAE: 11.8887, RMSE: 24.6854, Non-zero MAE: 35.8819, Non-zero RMSE: 49.4722, Intent Accuracy: 0.1667
Position 22 - MAE: 11.5599, RMSE: 24.6995, Non-zero MAE: 34.4108, Non-zero RMSE: 47.9521, Intent Accuracy: 0.1429
Position 23 - MAE: 11.3508, RMSE: 24.4445, Non-zero MAE: 33.5111, Non-zero RMSE: 46.6205, Intent Accuracy: 0.1667
Position 24 - MAE: 10.9356, RMSE: 23.5402, Non-zero MAE: 32.3517, Non-zero RMSE: 44.1359, Intent Accuracy: 0.1429
Position 25 - MAE: 10.7391, RMSE: 23.8334, Non-zero MAE: 32.6535, Non-zero RMSE: 46.6816, Intent Accuracy: 0.1429
Position 26 - MAE: 10.2969, RMSE: 22.9664, Non-zero MAE: 30.0476, Non-zero RMSE: 42.6072, Intent Accuracy: 0.1667
Position 27 - MAE: 10.4780, RMSE: 24.4220, Non-zero MAE: 30.7435, Non-zero RMSE: 44.7042, Intent Accuracy: 0.1429
Position 28 - MAE: 10.5278, RMSE: 25.6513, Non-zero MAE: 28.9657, Non-zero RMSE: 43.5994, Intent Accuracy: 0.1429
Position 29 - MAE: 10.4279, RMSE: 25.7098, Non-zero MAE: 28.0134, Non-zero RMSE: 41.7563, Intent Accuracy: 0.1429

Execution time: 3.09 seconds
Max GPU memory usage: 22.49 MB

----------------------------------------

Running test for model: lstm, init_seq: 4
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/lstm_best_model.pth', min_delta=0.001, model_type='lstm', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/lstm_best_model.pth
Overall Inference Results - MAE: 9.3776, RMSE: 25.4823, Non-zero MAE: 38.9939, Non-zero RMSE: 53.0948, Intent Accuracy: 0.1905
Position 4 - MAE: 11.8812, RMSE: 16.9700, Non-zero MAE: 46.1070, Non-zero RMSE: 54.6437, Intent Accuracy: 0.1429
Position 5 - MAE: 12.6835, RMSE: 19.7939, Non-zero MAE: 53.4222, Non-zero RMSE: 70.6004, Intent Accuracy: 0.2619
Position 6 - MAE: 12.5702, RMSE: 19.3742, Non-zero MAE: 52.2832, Non-zero RMSE: 69.5093, Intent Accuracy: 0.2857
Position 7 - MAE: 12.1927, RMSE: 18.5156, Non-zero MAE: 50.7516, Non-zero RMSE: 67.2954, Intent Accuracy: 0.3095
Position 8 - MAE: 11.4676, RMSE: 17.9249, Non-zero MAE: 49.2207, Non-zero RMSE: 64.9097, Intent Accuracy: 0.4762
Position 9 - MAE: 11.6880, RMSE: 18.9265, Non-zero MAE: 51.1992, Non-zero RMSE: 68.3066, Intent Accuracy: 0.1429
Position 10 - MAE: 11.4311, RMSE: 17.9775, Non-zero MAE: 48.4877, Non-zero RMSE: 64.4429, Intent Accuracy: 0.0238
Position 11 - MAE: 10.8801, RMSE: 16.9984, Non-zero MAE: 47.4096, Non-zero RMSE: 62.2492, Intent Accuracy: 0.0000
Position 12 - MAE: 10.6156, RMSE: 16.7629, Non-zero MAE: 46.1233, Non-zero RMSE: 61.4721, Intent Accuracy: 0.0000
Position 13 - MAE: 10.3506, RMSE: 16.0928, Non-zero MAE: 43.9835, Non-zero RMSE: 58.7528, Intent Accuracy: 0.0476
Position 14 - MAE: 10.1956, RMSE: 15.8356, Non-zero MAE: 42.2040, Non-zero RMSE: 55.6030, Intent Accuracy: 0.1190
Position 15 - MAE: 9.9419, RMSE: 15.5796, Non-zero MAE: 41.6973, Non-zero RMSE: 56.4003, Intent Accuracy: 0.1429
Position 16 - MAE: 9.5345, RMSE: 14.9842, Non-zero MAE: 40.3253, Non-zero RMSE: 53.6613, Intent Accuracy: 0.1429
Position 17 - MAE: 9.3082, RMSE: 14.7501, Non-zero MAE: 39.0932, Non-zero RMSE: 52.7902, Intent Accuracy: 0.1429
Position 18 - MAE: 8.8104, RMSE: 13.5247, Non-zero MAE: 37.0443, Non-zero RMSE: 48.5841, Intent Accuracy: 0.1429
Position 19 - MAE: 8.9234, RMSE: 13.7514, Non-zero MAE: 36.2397, Non-zero RMSE: 48.3538, Intent Accuracy: 0.1905
Position 20 - MAE: 8.4351, RMSE: 13.1858, Non-zero MAE: 35.0942, Non-zero RMSE: 46.9361, Intent Accuracy: 0.1429
Position 21 - MAE: 8.1953, RMSE: 12.8993, Non-zero MAE: 33.7739, Non-zero RMSE: 45.5101, Intent Accuracy: 0.2381
Position 22 - MAE: 7.6741, RMSE: 12.1544, Non-zero MAE: 31.8080, Non-zero RMSE: 42.4741, Intent Accuracy: 0.2381
Position 23 - MAE: 7.4718, RMSE: 11.6692, Non-zero MAE: 30.9091, Non-zero RMSE: 41.3033, Intent Accuracy: 0.0952
Position 24 - MAE: 7.2037, RMSE: 11.1118, Non-zero MAE: 29.7044, Non-zero RMSE: 39.4101, Intent Accuracy: 0.1429
Position 25 - MAE: 6.8638, RMSE: 10.7644, Non-zero MAE: 28.5193, Non-zero RMSE: 38.7456, Intent Accuracy: 0.0952
Position 26 - MAE: 6.5534, RMSE: 10.0919, Non-zero MAE: 26.1435, Non-zero RMSE: 35.0157, Intent Accuracy: 0.0952
Position 27 - MAE: 6.3511, RMSE: 9.8794, Non-zero MAE: 25.5577, Non-zero RMSE: 34.4114, Intent Accuracy: 0.1429
Position 28 - MAE: 5.9767, RMSE: 9.2360, Non-zero MAE: 24.1961, Non-zero RMSE: 32.9517, Intent Accuracy: 0.1905
Position 29 - MAE: 5.7349, RMSE: 8.9586, Non-zero MAE: 22.8695, Non-zero RMSE: 31.0913, Intent Accuracy: 0.1905

Execution time: 3.21 seconds
Max GPU memory usage: 38.20 MB

----------------------------------------

Running test for model: vanillatf, init_seq: 4
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/vanillatf_best_model.pth', min_delta=0.001, model_type='vanillatf', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/vanillatf_best_model.pth
Overall Inference Results - MAE: 9.5563, RMSE: 22.9869, Non-zero MAE: 33.3173, Non-zero RMSE: 46.7931, Intent Accuracy: 0.1667
Position 4 - MAE: 11.9832, RMSE: 16.1278, Non-zero MAE: 26.8908, Non-zero RMSE: 35.9591, Intent Accuracy: 0.7619
Position 5 - MAE: 12.7014, RMSE: 18.6878, Non-zero MAE: 40.5245, Non-zero RMSE: 56.8936, Intent Accuracy: 0.5952
Position 6 - MAE: 12.2745, RMSE: 18.2131, Non-zero MAE: 45.3093, Non-zero RMSE: 61.3409, Intent Accuracy: 0.5714
Position 7 - MAE: 12.0445, RMSE: 17.7615, Non-zero MAE: 46.3029, Non-zero RMSE: 61.7322, Intent Accuracy: 0.5476
Position 8 - MAE: 11.5546, RMSE: 17.1732, Non-zero MAE: 42.7709, Non-zero RMSE: 57.8859, Intent Accuracy: 0.5238
Position 9 - MAE: 11.2858, RMSE: 17.0104, Non-zero MAE: 41.3064, Non-zero RMSE: 57.0823, Intent Accuracy: 0.5238
Position 10 - MAE: 11.1383, RMSE: 16.6324, Non-zero MAE: 41.1693, Non-zero RMSE: 56.1556, Intent Accuracy: 0.4286
Position 11 - MAE: 10.6307, RMSE: 15.8399, Non-zero MAE: 40.9616, Non-zero RMSE: 55.2784, Intent Accuracy: 0.3571
Position 12 - MAE: 10.3722, RMSE: 15.5163, Non-zero MAE: 39.9542, Non-zero RMSE: 54.4809, Intent Accuracy: 0.3810
Position 13 - MAE: 10.0101, RMSE: 14.8795, Non-zero MAE: 38.4018, Non-zero RMSE: 52.2828, Intent Accuracy: 0.3810
Position 14 - MAE: 9.7802, RMSE: 14.4524, Non-zero MAE: 36.8214, Non-zero RMSE: 49.2727, Intent Accuracy: 0.3571
Position 15 - MAE: 9.7796, RMSE: 14.6680, Non-zero MAE: 37.1306, Non-zero RMSE: 51.1912, Intent Accuracy: 0.3095
Position 16 - MAE: 9.4427, RMSE: 14.1218, Non-zero MAE: 35.4603, Non-zero RMSE: 48.3600, Intent Accuracy: 0.3810
Position 17 - MAE: 9.2944, RMSE: 14.1152, Non-zero MAE: 34.9345, Non-zero RMSE: 48.2137, Intent Accuracy: 0.3333
Position 18 - MAE: 8.7529, RMSE: 13.0535, Non-zero MAE: 33.6155, Non-zero RMSE: 44.9002, Intent Accuracy: 0.3333
Position 19 - MAE: 8.9985, RMSE: 13.4900, Non-zero MAE: 33.2884, Non-zero RMSE: 45.4276, Intent Accuracy: 0.2619
Position 20 - MAE: 8.6202, RMSE: 13.0036, Non-zero MAE: 31.4004, Non-zero RMSE: 43.4906, Intent Accuracy: 0.2143
Position 21 - MAE: 8.5101, RMSE: 12.8874, Non-zero MAE: 29.9762, Non-zero RMSE: 41.7834, Intent Accuracy: 0.2857
Position 22 - MAE: 8.0353, RMSE: 12.1444, Non-zero MAE: 27.9049, Non-zero RMSE: 38.7679, Intent Accuracy: 0.2381
Position 23 - MAE: 7.8665, RMSE: 11.8685, Non-zero MAE: 26.9426, Non-zero RMSE: 37.4184, Intent Accuracy: 0.2381
Position 24 - MAE: 7.5974, RMSE: 11.4337, Non-zero MAE: 26.0917, Non-zero RMSE: 35.6099, Intent Accuracy: 0.2143
Position 25 - MAE: 7.3230, RMSE: 11.0835, Non-zero MAE: 24.9268, Non-zero RMSE: 35.1731, Intent Accuracy: 0.2619
Position 26 - MAE: 6.8872, RMSE: 10.3590, Non-zero MAE: 22.8847, Non-zero RMSE: 31.7138, Intent Accuracy: 0.2143
Position 27 - MAE: 6.7830, RMSE: 10.2244, Non-zero MAE: 22.2830, Non-zero RMSE: 31.2717, Intent Accuracy: 0.1905
Position 28 - MAE: 6.5389, RMSE: 9.8194, Non-zero MAE: 20.7333, Non-zero RMSE: 29.5555, Intent Accuracy: 0.2143
Position 29 - MAE: 6.4398, RMSE: 9.8643, Non-zero MAE: 19.6403, Non-zero RMSE: 28.0723, Intent Accuracy: 0.1667

Execution time: 2.52 seconds
Max GPU memory usage: 125.06 MB

----------------------------------------

Running test for model: tabbert, init_seq: 4
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabbert_best_model.pth', min_delta=0.001, model_type='tabbert', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabbert_best_model.pth
Overall Inference Results - MAE: 9.8852, RMSE: 23.6766, Non-zero MAE: 33.7261, Non-zero RMSE: 48.4352, Intent Accuracy: 0.2381
Position 4 - MAE: 11.6171, RMSE: 15.7883, Non-zero MAE: 29.5581, Non-zero RMSE: 38.2440, Intent Accuracy: 0.9762
Position 5 - MAE: 12.0944, RMSE: 18.4669, Non-zero MAE: 45.4075, Non-zero RMSE: 61.9011, Intent Accuracy: 0.7381
Position 6 - MAE: 12.7921, RMSE: 19.4104, Non-zero MAE: 48.2766, Non-zero RMSE: 65.7689, Intent Accuracy: 0.5238
Position 7 - MAE: 12.7726, RMSE: 19.1860, Non-zero MAE: 48.5761, Non-zero RMSE: 65.5162, Intent Accuracy: 0.3571
Position 8 - MAE: 12.4261, RMSE: 18.9654, Non-zero MAE: 46.0574, Non-zero RMSE: 63.1049, Intent Accuracy: 0.3333
Position 9 - MAE: 12.2253, RMSE: 18.9994, Non-zero MAE: 45.1974, Non-zero RMSE: 62.9830, Intent Accuracy: 0.3333
Position 10 - MAE: 11.9493, RMSE: 18.1780, Non-zero MAE: 42.5599, Non-zero RMSE: 59.2183, Intent Accuracy: 0.3095
Position 11 - MAE: 11.4616, RMSE: 17.2841, Non-zero MAE: 41.6253, Non-zero RMSE: 57.0392, Intent Accuracy: 0.3095
Position 12 - MAE: 11.0999, RMSE: 16.9295, Non-zero MAE: 40.4196, Non-zero RMSE: 56.4809, Intent Accuracy: 0.3095
Position 13 - MAE: 10.8319, RMSE: 16.3911, Non-zero MAE: 38.9047, Non-zero RMSE: 54.3149, Intent Accuracy: 0.3095
Position 14 - MAE: 10.6906, RMSE: 16.0799, Non-zero MAE: 37.4168, Non-zero RMSE: 51.2412, Intent Accuracy: 0.2857
Position 15 - MAE: 10.4833, RMSE: 15.9455, Non-zero MAE: 36.8348, Non-zero RMSE: 52.1429, Intent Accuracy: 0.2857
Position 16 - MAE: 10.0956, RMSE: 15.3262, Non-zero MAE: 35.3351, Non-zero RMSE: 49.2313, Intent Accuracy: 0.2857
Position 17 - MAE: 9.8977, RMSE: 15.1158, Non-zero MAE: 34.2003, Non-zero RMSE: 48.5028, Intent Accuracy: 0.2857
Position 18 - MAE: 9.3375, RMSE: 13.8937, Non-zero MAE: 32.2387, Non-zero RMSE: 44.3919, Intent Accuracy: 0.2619
Position 19 - MAE: 9.4269, RMSE: 14.0949, Non-zero MAE: 31.6202, Non-zero RMSE: 44.3404, Intent Accuracy: 0.2857
Position 20 - MAE: 9.0506, RMSE: 13.6788, Non-zero MAE: 30.4415, Non-zero RMSE: 42.7864, Intent Accuracy: 0.2857
Position 21 - MAE: 8.8162, RMSE: 13.3779, Non-zero MAE: 29.1336, Non-zero RMSE: 41.4503, Intent Accuracy: 0.2619
Position 22 - MAE: 8.3057, RMSE: 12.6925, Non-zero MAE: 27.1571, Non-zero RMSE: 38.4354, Intent Accuracy: 0.2381
Position 23 - MAE: 8.0530, RMSE: 12.1493, Non-zero MAE: 26.2840, Non-zero RMSE: 37.1209, Intent Accuracy: 0.2381
Position 24 - MAE: 7.8020, RMSE: 11.6263, Non-zero MAE: 25.1697, Non-zero RMSE: 35.3806, Intent Accuracy: 0.2381
Position 25 - MAE: 7.4967, RMSE: 11.3594, Non-zero MAE: 24.0434, Non-zero RMSE: 34.7562, Intent Accuracy: 0.2381
Position 26 - MAE: 7.0982, RMSE: 10.5961, Non-zero MAE: 21.8327, Non-zero RMSE: 31.1271, Intent Accuracy: 0.2381
Position 27 - MAE: 6.9721, RMSE: 10.4624, Non-zero MAE: 21.2723, Non-zero RMSE: 30.5500, Intent Accuracy: 0.2381
Position 28 - MAE: 6.6271, RMSE: 9.9332, Non-zero MAE: 19.9719, Non-zero RMSE: 29.2593, Intent Accuracy: 0.2381
Position 29 - MAE: 6.4285, RMSE: 9.7419, Non-zero MAE: 18.6882, Non-zero RMSE: 27.3926, Intent Accuracy: 0.2381

Execution time: 2.40 seconds
Max GPU memory usage: 270.64 MB

----------------------------------------

Running test for model: tabgpt2, init_seq: 4
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/config_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=4, intermediate_size=2400, learning_rate=0.0001, load_path='./training_results/tabgpt2_best_model.pth', min_delta=0.001, model_type='tabgpt2', noise_mean=0, noise_std=0.1, num_epochs=300, num_heads=4, num_layers=2, patience=20, save_path='best_model.pth', seed=0, seq_num_heads=16, seq_num_layers=2, swap_prob=0.2, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabgpt2_best_model.pth
Overall Inference Results - MAE: 9.6788, RMSE: 24.7562, Non-zero MAE: 36.3981, Non-zero RMSE: 51.2373, Intent Accuracy: 0.1429
Position 4 - MAE: 12.0535, RMSE: 16.7507, Non-zero MAE: 38.3664, Non-zero RMSE: 47.9269, Intent Accuracy: 0.4048
Position 5 - MAE: 12.6164, RMSE: 19.5153, Non-zero MAE: 51.8150, Non-zero RMSE: 68.7459, Intent Accuracy: 0.5476
Position 6 - MAE: 12.5313, RMSE: 19.3546, Non-zero MAE: 52.5522, Non-zero RMSE: 69.7678, Intent Accuracy: 0.4048
Position 7 - MAE: 12.6130, RMSE: 18.9049, Non-zero MAE: 50.7941, Non-zero RMSE: 67.9583, Intent Accuracy: 0.2143
Position 8 - MAE: 12.3824, RMSE: 18.6332, Non-zero MAE: 47.6404, Non-zero RMSE: 65.0014, Intent Accuracy: 0.2619
Position 9 - MAE: 12.0875, RMSE: 18.9241, Non-zero MAE: 47.1135, Non-zero RMSE: 65.0577, Intent Accuracy: 0.1429
Position 10 - MAE: 11.9004, RMSE: 18.2128, Non-zero MAE: 43.7742, Non-zero RMSE: 60.9528, Intent Accuracy: 0.1667
Position 11 - MAE: 11.2507, RMSE: 17.2014, Non-zero MAE: 44.2213, Non-zero RMSE: 59.8047, Intent Accuracy: 0.0714
Position 12 - MAE: 10.9572, RMSE: 16.8986, Non-zero MAE: 42.8437, Non-zero RMSE: 58.9151, Intent Accuracy: 0.0714
Position 13 - MAE: 10.6183, RMSE: 16.2665, Non-zero MAE: 41.2962, Non-zero RMSE: 56.6472, Intent Accuracy: 0.0476
Position 14 - MAE: 10.4160, RMSE: 15.7809, Non-zero MAE: 39.3258, Non-zero RMSE: 53.2860, Intent Accuracy: 0.1190
Position 15 - MAE: 10.2012, RMSE: 15.6488, Non-zero MAE: 39.0253, Non-zero RMSE: 54.3999, Intent Accuracy: 0.1429
Position 16 - MAE: 9.8182, RMSE: 15.0897, Non-zero MAE: 37.4669, Non-zero RMSE: 51.5690, Intent Accuracy: 0.1190
Position 17 - MAE: 9.5292, RMSE: 14.7544, Non-zero MAE: 36.4925, Non-zero RMSE: 50.9044, Intent Accuracy: 0.1190
Position 18 - MAE: 9.0060, RMSE: 13.5783, Non-zero MAE: 34.8737, Non-zero RMSE: 47.0409, Intent Accuracy: 0.1190
Position 19 - MAE: 9.1138, RMSE: 13.8292, Non-zero MAE: 34.3360, Non-zero RMSE: 46.9289, Intent Accuracy: 0.1190
Position 20 - MAE: 8.6417, RMSE: 13.2609, Non-zero MAE: 32.9086, Non-zero RMSE: 45.2645, Intent Accuracy: 0.0714
Position 21 - MAE: 8.5337, RMSE: 13.0981, Non-zero MAE: 31.2744, Non-zero RMSE: 43.7898, Intent Accuracy: 0.0952
Position 22 - MAE: 7.9933, RMSE: 12.3545, Non-zero MAE: 29.2411, Non-zero RMSE: 40.5648, Intent Accuracy: 0.1190
Position 23 - MAE: 7.7363, RMSE: 11.8526, Non-zero MAE: 28.5273, Non-zero RMSE: 39.5472, Intent Accuracy: 0.0714
Position 24 - MAE: 7.4346, RMSE: 11.1984, Non-zero MAE: 27.3360, Non-zero RMSE: 37.6286, Intent Accuracy: 0.1429
Position 25 - MAE: 7.1354, RMSE: 10.9482, Non-zero MAE: 26.2715, Non-zero RMSE: 37.1145, Intent Accuracy: 0.1190
Position 26 - MAE: 6.8552, RMSE: 10.3406, Non-zero MAE: 23.9591, Non-zero RMSE: 33.3965, Intent Accuracy: 0.0476
Position 27 - MAE: 6.6404, RMSE: 10.0622, Non-zero MAE: 23.2017, Non-zero RMSE: 32.7505, Intent Accuracy: 0.0952
Position 28 - MAE: 6.3516, RMSE: 9.5941, Non-zero MAE: 21.8729, Non-zero RMSE: 31.3114, Intent Accuracy: 0.1190
Position 29 - MAE: 6.1662, RMSE: 9.3861, Non-zero MAE: 20.6996, Non-zero RMSE: 29.6533, Intent Accuracy: 0.1429

Execution time: 4.13 seconds
Max GPU memory usage: 326.13 MB

----------------------------------------

Running test for model: tabllama, init_seq: 4
/home/emforce77/anaconda3/envs/pret/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/home/emforce77/src/utils.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
Data dimension: 300, Max sequence length: 30, num_intent: 7
intent_mapping {'재래식 전면공격 여건조성/아군의 전구작전 수행능력 저하_미사일방어체계': 0, '재래식 전면공격 여건조성/아군의 전쟁지도 및 지휘능력 마비_전쟁지휘통제시설': 1, '재래식 전면공격 여건조성/아군의 전쟁지속 및 전시증원능력 마비_전쟁지휘통제시설, 전쟁지속지원능력': 2, '재래식 전면공격 여건조성/전쟁 수행의지 약화 및 사회적 혼란 조성_정치/심리적표적': 3, '핵 공격 여건조성/한국의 거부적 억제수단 무력화_작전적 타격수단, 미사일방어체계': 4, '핵 공격 여건조성/한국의 보복적 억제수단 무력화_전략적 타격수단': 5, '핵 공격 여건조성/한미 연합작전 수행능력 무력화_미사일방어체계, 한미연합전력': 6}
args Namespace(batch_size=32, col_dim=5, config='./configs/utils/default_infer.yaml', debug=False, device='cuda', file_path='./multiple_scenario_data_infer.xml', hidden_size=4, in_dist=False, infer=True, init_seq=4, intermediate_size=2400, learning_rate=2.491423889455553e-05, load_path='./training_results/tabllama_best_model.pth', min_delta=0.001, model_type='tabllama', noise_mean=0, noise_std=0.022400575875951165, num_epochs=300, num_heads=4, num_layers=2, patience=15, save_path='best_model.pth', seed=0, seq_num_heads=4, seq_num_layers=3, swap_prob=0.0083388429783301, test=False, test_swap=False, train=False, use_augment_swap=True, use_augment_warp=True, use_random_noise=True, warp_limit=20)
Data sizes: train=5684, val=1218, test=1218
Model parameters: col_dim=5, region_len=60, sequence_len=30, num_classes=445
Loading model from: ./training_results/tabllama_best_model.pth
Overall Inference Results - MAE: 12.6918, RMSE: 24.5127, Non-zero MAE: 26.6919, Non-zero RMSE: 41.4943, Intent Accuracy: 0.1667
Position 4 - MAE: 11.5855, RMSE: 15.7918, Non-zero MAE: 30.0248, Non-zero RMSE: 38.4566, Intent Accuracy: 0.9286
Position 5 - MAE: 13.1253, RMSE: 18.8457, Non-zero MAE: 33.4292, Non-zero RMSE: 49.5359, Intent Accuracy: 0.7143
Position 6 - MAE: 13.7557, RMSE: 19.7377, Non-zero MAE: 35.3250, Non-zero RMSE: 52.5400, Intent Accuracy: 0.4762
Position 7 - MAE: 14.1655, RMSE: 20.4004, Non-zero MAE: 35.7926, Non-zero RMSE: 52.7475, Intent Accuracy: 0.3571
Position 8 - MAE: 13.6932, RMSE: 20.1115, Non-zero MAE: 33.8230, Non-zero RMSE: 50.9069, Intent Accuracy: 0.4048
Position 9 - MAE: 14.5576, RMSE: 21.7773, Non-zero MAE: 35.3008, Non-zero RMSE: 53.9197, Intent Accuracy: 0.2619
Position 10 - MAE: 14.2584, RMSE: 21.1932, Non-zero MAE: 33.0129, Non-zero RMSE: 50.2913, Intent Accuracy: 0.2381
Position 11 - MAE: 13.9726, RMSE: 20.7149, Non-zero MAE: 32.4229, Non-zero RMSE: 48.9487, Intent Accuracy: 0.2619
Position 12 - MAE: 13.8910, RMSE: 20.7749, Non-zero MAE: 31.8635, Non-zero RMSE: 49.1295, Intent Accuracy: 0.2619
Position 13 - MAE: 13.7916, RMSE: 20.8850, Non-zero MAE: 30.7829, Non-zero RMSE: 47.4402, Intent Accuracy: 0.1429
Position 14 - MAE: 13.5864, RMSE: 20.6011, Non-zero MAE: 29.2351, Non-zero RMSE: 44.1659, Intent Accuracy: 0.1667
Position 15 - MAE: 13.3894, RMSE: 20.4382, Non-zero MAE: 29.0523, Non-zero RMSE: 45.5650, Intent Accuracy: 0.1429
Position 16 - MAE: 13.0940, RMSE: 19.8809, Non-zero MAE: 27.8017, Non-zero RMSE: 42.6190, Intent Accuracy: 0.1667
Position 17 - MAE: 13.0876, RMSE: 20.1125, Non-zero MAE: 27.2596, Non-zero RMSE: 42.5137, Intent Accuracy: 0.1667
Position 18 - MAE: 12.5571, RMSE: 19.1407, Non-zero MAE: 25.5544, Non-zero RMSE: 38.5280, Intent Accuracy: 0.1905
Position 19 - MAE: 12.5902, RMSE: 19.3129, Non-zero MAE: 25.2397, Non-zero RMSE: 38.6060, Intent Accuracy: 0.1429
Position 20 - MAE: 12.6252, RMSE: 19.6496, Non-zero MAE: 24.3054, Non-zero RMSE: 37.3262, Intent Accuracy: 0.1429
Position 21 - MAE: 12.2958, RMSE: 18.9175, Non-zero MAE: 23.0708, Non-zero RMSE: 35.9442, Intent Accuracy: 0.1905
Position 22 - MAE: 11.8794, RMSE: 18.5262, Non-zero MAE: 21.4338, Non-zero RMSE: 33.2344, Intent Accuracy: 0.1905
Position 23 - MAE: 11.6846, RMSE: 18.0935, Non-zero MAE: 21.0874, Non-zero RMSE: 32.8587, Intent Accuracy: 0.1667
Position 24 - MAE: 11.4879, RMSE: 17.8115, Non-zero MAE: 19.7982, Non-zero RMSE: 30.7306, Intent Accuracy: 0.1667
Position 25 - MAE: 11.4174, RMSE: 17.9592, Non-zero MAE: 19.4478, Non-zero RMSE: 30.5868, Intent Accuracy: 0.1429
Position 26 - MAE: 11.1320, RMSE: 17.9790, Non-zero MAE: 18.2001, Non-zero RMSE: 28.2588, Intent Accuracy: 0.1429
Position 27 - MAE: 11.0341, RMSE: 17.6431, Non-zero MAE: 17.9197, Non-zero RMSE: 27.8377, Intent Accuracy: 0.1429
Position 28 - MAE: 10.8799, RMSE: 17.3770, Non-zero MAE: 17.0648, Non-zero RMSE: 26.6352, Intent Accuracy: 0.1429
Position 29 - MAE: 10.6841, RMSE: 17.0791, Non-zero MAE: 16.0596, Non-zero RMSE: 25.1696, Intent Accuracy: 0.1667

Execution time: 4.07 seconds
Max GPU memory usage: 337.77 MB

----------------------------------------

